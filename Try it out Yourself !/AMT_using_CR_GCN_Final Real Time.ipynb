{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSG_uu0QXgc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import IPython\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required system packages and Python dependencies.\"\"\"\n",
        "    os.system(\"apt-get update -qq && apt-get install -qq libfluidsynth3 build-essential libasound2-dev libjack-dev\")\n",
        "    os.system(\"git clone --branch=main https://github.com/magenta/mt3\")\n",
        "    os.system(\"mv mt3 mt3_tmp; mv mt3_tmp/* .; rm -r mt3_tmp\")\n",
        "    os.system(\"python3 -m pip install jax[cuda12] nest-asyncio pyfluidsynth==1.3.0 -e . -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "\n",
        "def copy_checkpoints(checkpoint_dir='checkpoints'):\n",
        "    \"\"\"Copy model checkpoints from cloud storage.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    os.system(f\"gsutil -q -m cp -r gs://mt3/checkpoints {checkpoint_dir}\")\n",
        "\n",
        "def copy_soundfont(soundfont_path='SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'):\n",
        "    \"\"\"Copy soundfont file for audio synthesis.\"\"\"\n",
        "    os.makedirs(os.path.dirname(soundfont_path), exist_ok=True)\n",
        "    os.system(f\"gsutil -q -m cp gs://magentadata/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 {soundfont_path}\")\n",
        "\n",
        "def AnalyticsSetup(analytics_id='G-4P250YRJ08'):\n",
        "    \"\"\"Set up anonymous analytics tracking.\"\"\"\n",
        "    html_code = f'''\n",
        "<!-- Analytics Setup -->\n",
        "<script async src=\"https://www.googletagmanager.com/gtag/js?id={analytics_id}\"></script>\n",
        "<script>\n",
        "  window.dataLayer = window.dataLayer || [];\n",
        "  function gtag(){{dataLayer.push(arguments);}}\n",
        "  gtag('js', new Date());\n",
        "  gtag('config', '{analytics_id}',\n",
        "       {{'referrer': document.referrer.split('?')[0],\n",
        "        'anonymize_ip': true,\n",
        "        'page_title': '',\n",
        "        'page_referrer': '',\n",
        "        'cookie_prefix': 'magenta',\n",
        "        'cookie_domain': 'auto',\n",
        "        'cookie_expires': 0,\n",
        "        'cookie_flags': 'SameSite=None;Secure'}});\n",
        "</script>\n",
        "'''\n",
        "    IPython.display.display(IPython.display.HTML(html_code))\n",
        "\n",
        "def LogAnalyticsEvent(event_name, event_details):\n",
        "    \"\"\"Log an analytics event with specified details.\"\"\"\n",
        "    details_json = json.dumps(event_details)\n",
        "    js_string = f\"gtag('event', '{event_name}', {details_json});\"\n",
        "    IPython.display.display(IPython.display.Javascript(js_string))\n",
        "\n",
        "# Original working lines\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth3 build-essential libasound2-dev libjack-dev\n",
        "!git clone --branch=main https://github.com/magenta/mt3\n",
        "!mv mt3 mt3_tmp; mv mt3_tmp/* .; rm -r mt3_tmp\n",
        "!python3 -m pip install jax[cuda12] nest-asyncio pyfluidsynth==1.3.0 -e . -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!gsutil -q -m cp -r gs://mt3/checkpoints .\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 .\n",
        "\n",
        "import json\n",
        "import IPython\n",
        "\n",
        "def load_gtag():\n",
        "  \"\"\"Loads gtag.js.\"\"\"\n",
        "  html_code = '''\n",
        "<!-- Global site tag (gtag.js) - Google Analytics -->\n",
        "<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-4P250YRJ08\"></script>\n",
        "<script>\n",
        "  window.dataLayer = window.dataLayer || [];\n",
        "  function gtag(){dataLayer.push(arguments);}\n",
        "  gtag('js', new Date());\n",
        "  gtag('config', 'G-4P250YRJ08',\n",
        "       {'referrer': document.referrer.split('?')[0],\n",
        "        'anonymize_ip': true,\n",
        "        'page_title': '',\n",
        "        'page_referrer': '',\n",
        "        'cookie_prefix': 'magenta',\n",
        "        'cookie_domain': 'auto',\n",
        "        'cookie_expires': 0,\n",
        "        'cookie_flags': 'SameSite=None;Secure'});\n",
        "</script>\n",
        "'''\n",
        "  IPython.display.display(IPython.display.HTML(html_code))\n",
        "\n",
        "def log_event(event_name, event_details):\n",
        "  \"\"\"Log event with name and details dictionary.\"\"\"\n",
        "  details_json = json.dumps(event_details)\n",
        "  js_string = \"gtag('event', '%s', %s);\" % (event_name, details_json)\n",
        "  IPython.display.display(IPython.display.Javascript(js_string))\n",
        "\n",
        "load_gtag()\n",
        "log_event('setupComplete', {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSMSWDxxWmTS"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import gin\n",
        "import jax\n",
        "import librosa\n",
        "import note_seq\n",
        "import seqio\n",
        "import t5\n",
        "import t5x\n",
        "from mt3 import metrics_utils, models, network, note_sequences, preprocessors, spectrograms, vocabularies\n",
        "from google.colab import files\n",
        "import nest_asyncio\n",
        "\n",
        "def initialize_transcription_config(model_type='mt3', batch_size=8, inputs_length=256, outputs_length=1024):\n",
        "    \"\"\"Initialize configuration for transcription model.\"\"\"\n",
        "    nest_asyncio.apply()\n",
        "    if model_type == 'ismir2021':\n",
        "        num_velocity_bins = 127\n",
        "        encoding_spec = note_sequences.NoteEncodingSpec\n",
        "        inputs_length = 512\n",
        "    elif model_type == 'mt3':\n",
        "        num_velocity_bins = 1\n",
        "        encoding_spec = note_sequences.NoteEncodingWithTiesSpec\n",
        "    else:\n",
        "        raise ValueError(f'Unknown model_type: {model_type}')\n",
        "\n",
        "    sequence_length = {'inputs': inputs_length, 'targets': outputs_length}\n",
        "    spectrogram_config = spectrograms.SpectrogramConfig()\n",
        "    codec = vocabularies.build_codec(vocab_config=vocabularies.VocabularyConfig(num_velocity_bins=num_velocity_bins))\n",
        "    vocabulary = vocabularies.vocabulary_from_codec(codec)\n",
        "    output_features = {\n",
        "        'inputs': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
        "        'targets': seqio.Feature(vocabulary=vocabulary),\n",
        "    }\n",
        "    return {\n",
        "        'batch_size': batch_size,\n",
        "        'sequence_length': sequence_length,\n",
        "        'spectrogram_config': spectrogram_config,\n",
        "        'codec': codec,\n",
        "        'vocabulary': vocabulary,\n",
        "        'output_features': output_features,\n",
        "        'encoding_spec': encoding_spec\n",
        "    }\n",
        "\n",
        "def parse_gin_config(gin_files, num_velocity_bins):\n",
        "    \"\"\"Parse gin configuration files.\"\"\"\n",
        "    gin_bindings = [\n",
        "        'from __gin__ import dynamic_registration',\n",
        "        'from mt3 import vocabularies',\n",
        "        'VOCAB_CONFIG=@vocabularies.VocabularyConfig()',\n",
        "        f'vocabularies.VocabularyConfig.num_velocity_bins={num_velocity_bins}'\n",
        "    ]\n",
        "    with gin.unlock_config():\n",
        "        gin.parse_config_files_and_bindings(gin_files, gin_bindings, finalize_config=False)\n",
        "\n",
        "def BuildTranscriptionModel(config):\n",
        "    \"\"\"Build the transcription model.\"\"\"\n",
        "    model_config = gin.get_configurable(network.T5Config)()\n",
        "    module = network.Transformer(config=model_config)\n",
        "    return t5x.models.ContinuousInputsEncoderDecoderModel(\n",
        "        module=module,\n",
        "        input_vocabulary=config['output_features']['inputs'].vocabulary,\n",
        "        output_vocabulary=config['output_features']['targets'].vocabulary,\n",
        "        optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
        "        input_depth=spectrograms.input_depth(config['spectrogram_config'])\n",
        "    )\n",
        "\n",
        "def restore_model_from_checkpoint(model, checkpoint_path, config):\n",
        "    \"\"\"Restore model state from checkpoint.\"\"\"\n",
        "    partitioner = t5x.partitioning.PjitPartitioner(num_partitions=1)\n",
        "    input_shapes = {\n",
        "        'encoder_input_tokens': (config['batch_size'], config['sequence_length']['inputs']),\n",
        "        'decoder_input_tokens': (config['batch_size'], config['sequence_length']['targets'])\n",
        "    }\n",
        "    train_state_initializer = t5x.utils.TrainStateInitializer(\n",
        "        optimizer_def=model.optimizer_def,\n",
        "        init_fn=model.get_initial_variables,\n",
        "        input_shapes=input_shapes,\n",
        "        partitioner=partitioner\n",
        "    )\n",
        "    restore_checkpoint_cfg = t5x.utils.RestoreCheckpointConfig(path=checkpoint_path, mode='specific', dtype='float32')\n",
        "    train_state_axes = train_state_initializer.train_state_axes\n",
        "    predict_fn = partitioner.partition(\n",
        "        lambda params, batch, decode_rng: model.predict_batch_with_aux(params, batch, decoder_params={'decode_rng': None}),\n",
        "        in_axis_resources=(train_state_axes.params, t5x.partitioning.PartitionSpec('data',), None),\n",
        "        out_axis_resources=t5x.partitioning.PartitionSpec('data',)\n",
        "    )\n",
        "    train_state = train_state_initializer.from_checkpoint_or_scratch(\n",
        "        [restore_checkpoint_cfg], init_rng=jax.random.PRNGKey(0)\n",
        "    )\n",
        "    return predict_fn, train_state\n",
        "\n",
        "def preprocess_audio_to_dataset(audio, spectrogram_config):\n",
        "    \"\"\"Convert audio to dataset with spectrogram frames.\"\"\"\n",
        "    frame_size = spectrogram_config.hop_width\n",
        "    padding = [0, frame_size - len(audio) % frame_size]\n",
        "    audio = np.pad(audio, padding, mode='constant')\n",
        "    frames = spectrograms.split_audio(audio, spectrogram_config)\n",
        "    num_frames = len(audio) // frame_size\n",
        "    times = np.arange(num_frames) / spectrogram_config.frames_per_second\n",
        "    return tf.data.Dataset.from_tensors({\n",
        "        'inputs': frames,\n",
        "        'input_times': times\n",
        "    })\n",
        "\n",
        "def preprocess_dataset(ds, config):\n",
        "    \"\"\"Preprocess dataset with spectrogram computation.\"\"\"\n",
        "    pp_chain = [\n",
        "        functools.partial(\n",
        "            t5.data.preprocessors.split_tokens_to_inputs_length,\n",
        "            sequence_length=config['sequence_length'],\n",
        "            output_features=config['output_features'],\n",
        "            feature_key='inputs',\n",
        "            additional_feature_keys=['input_times']\n",
        "        ),\n",
        "        preprocessors.add_dummy_targets,\n",
        "        functools.partial(\n",
        "            preprocessors.compute_spectrograms,\n",
        "            spectrogram_config=config['spectrogram_config']\n",
        "        )\n",
        "    ]\n",
        "    for pp in pp_chain:\n",
        "        ds = pp(ds)\n",
        "    return ds\n",
        "\n",
        "def predict_transcription_tokens(predict_fn, train_state, batch, seed=0):\n",
        "    \"\"\"Predict transcription tokens from dataset batch.\"\"\"\n",
        "    prediction, _ = predict_fn(train_state.params, batch, jax.random.PRNGKey(seed))\n",
        "    return prediction\n",
        "\n",
        "def postprocess_tokens(tokens, example, codec, encoding_spec):\n",
        "    \"\"\"Postprocess predicted tokens.\"\"\"\n",
        "    tokens = np.array(tokens, np.int32)\n",
        "    if vocabularies.DECODED_EOS_ID in tokens:\n",
        "        tokens = tokens[:np.argmax(tokens == vocabularies.DECODED_EOS_ID)]\n",
        "    start_time = example['input_times'][0]\n",
        "    start_time -= start_time % (1 / codec.steps_per_second)\n",
        "    return {\n",
        "        'est_tokens': tokens,\n",
        "        'start_time': start_time,\n",
        "        'raw_inputs': []\n",
        "    }\n",
        "\n",
        "def transcribe_audio(audio, predict_fn, train_state, config):\n",
        "    \"\"\"Transcribe audio to note sequence.\"\"\"\n",
        "    ds = preprocess_audio_to_dataset(audio, config['spectrogram_config'])\n",
        "    ds = preprocess_dataset(ds, config)\n",
        "    model_ds = config['output_features']['targets'].vocabulary.FEATURE_CONVERTER_CLS(pack=False)(\n",
        "        ds, task_feature_lengths=config['sequence_length']\n",
        "    )\n",
        "    model_ds = model_ds.batch(config['batch_size'])\n",
        "    inferences = (config['vocabulary'].decode_tf(tokens).numpy()\n",
        "                  for batch in model_ds.as_numpy_iterator()\n",
        "                  for tokens in predict_transcription_tokens(predict_fn, train_state, batch))\n",
        "    predictions = [postprocess_tokens(tokens, example, config['codec'], config['encoding_spec'])\n",
        "                   for example, tokens in zip(ds.as_numpy_iterator(), inferences)]\n",
        "    return metrics_utils.event_predictions_to_ns(predictions, codec=config['codec'], encoding_spec=config['encoding_spec'])['est_ns']\n",
        "\n",
        "def upload_audio_file(sample_rate=16000):\n",
        "    \"\"\"Upload and process audio file.\"\"\"\n",
        "    data = list(files.upload().values())\n",
        "    if len(data) > 1:\n",
        "        print('Multiple files uploaded; using only one.')\n",
        "    return note_seq.audio_io.wav_data_to_samples_librosa(data[0], sample_rate=sample_rate)\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import gin\n",
        "import jax\n",
        "import librosa\n",
        "import note_seq\n",
        "import seqio\n",
        "import t5\n",
        "import t5x\n",
        "from mt3 import metrics_utils\n",
        "from mt3 import models\n",
        "from mt3 import network\n",
        "from mt3 import note_sequences\n",
        "from mt3 import preprocessors\n",
        "from mt3 import spectrograms\n",
        "from mt3 import vocabularies\n",
        "from google.colab import files\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "SAMPLE_RATE = 16000\n",
        "SF2_PATH = 'SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'\n",
        "\n",
        "def upload_audio(sample_rate):\n",
        "  data = list(files.upload().values())\n",
        "  if len(data) > 1:\n",
        "    print('Multiple files uploaded; using only one.')\n",
        "  return note_seq.audio_io.wav_data_to_samples_librosa(\n",
        "    data[0], sample_rate=sample_rate)\n",
        "\n",
        "class InferenceModel(object):\n",
        "  \"\"\"Wrapper of T5X model for music transcription.\"\"\"\n",
        "  def __init__(self, checkpoint_path, model_type='mt3'):\n",
        "    if model_type == 'ismir2021':\n",
        "      num_velocity_bins = 127\n",
        "      self.encoding_spec = note_sequences.NoteEncodingSpec\n",
        "      self.inputs_length = 512\n",
        "    elif model_type == 'mt3':\n",
        "      num_velocity_bins = 1\n",
        "      self.encoding_spec = note_sequences.NoteEncodingWithTiesSpec\n",
        "      self.inputs_length = 256\n",
        "    else:\n",
        "      raise ValueError('unknown model_type: %s' % model_type)\n",
        "    self.gin_files = ['/content/mt3/gin/model.gin',  # Store gin_files as instance variable\n",
        "                     f'/content/mt3/gin/{model_type}.gin']\n",
        "    self.batch_size = 8\n",
        "    self.outputs_length = 1024\n",
        "    self.sequence_length = {'inputs': self.inputs_length,\n",
        "                            'targets': self.outputs_length}\n",
        "    self.partitioner = t5x.partitioning.PjitPartitioner(\n",
        "        num_partitions=1)\n",
        "\n",
        "    self.spectrogram_config = spectrograms.SpectrogramConfig()\n",
        "    self.codec = vocabularies.build_codec(\n",
        "        vocab_config=vocabularies.VocabularyConfig(\n",
        "            num_velocity_bins=num_velocity_bins))\n",
        "    self.vocabulary = vocabularies.vocabulary_from_codec(self.codec)\n",
        "    self.output_features = {\n",
        "        'inputs': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
        "        'targets': seqio.Feature(vocabulary=self.vocabulary),\n",
        "    }\n",
        "    self._parse_gin(self.gin_files)\n",
        "    self.model = self._load_model()\n",
        "    self.restore_from_checkpoint(checkpoint_path)\n",
        "  @property\n",
        "  def input_shapes(self):\n",
        "    return {\n",
        "          'encoder_input_tokens': (self.batch_size, self.inputs_length),\n",
        "          'decoder_input_tokens': (self.batch_size, self.outputs_length)\n",
        "    }\n",
        "  def _parse_gin(self, gin_files):\n",
        "    \"\"\"Parse gin files used to train the model.\"\"\"\n",
        "    gin_bindings = [\n",
        "        'from __gin__ import dynamic_registration',\n",
        "        'from mt3 import vocabularies',\n",
        "        'VOCAB_CONFIG=@vocabularies.VocabularyConfig()',\n",
        "        'vocabularies.VocabularyConfig.num_velocity_bins=%NUM_VELOCITY_BINS'\n",
        "    ]\n",
        "    with gin.unlock_config():\n",
        "      gin.parse_config_files_and_bindings(\n",
        "          gin_files, gin_bindings, finalize_config=False)\n",
        "  def _load_model(self):\n",
        "    \"\"\"Load up a T5X `Model` after parsing training gin config.\"\"\"\n",
        "    model_config = gin.get_configurable(network.T5Config)()\n",
        "    module = network.Transformer(config=model_config)\n",
        "    return models.ContinuousInputsEncoderDecoderModel(\n",
        "        module=module,\n",
        "        input_vocabulary=self.output_features['inputs'].vocabulary,\n",
        "        output_vocabulary=self.output_features['targets'].vocabulary,\n",
        "        optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
        "        input_depth=spectrograms.input_depth(self.spectrogram_config))\n",
        "  def restore_from_checkpoint(self, checkpoint_path):\n",
        "    \"\"\"Restore training state from checkpoint, resets self._predict_fn().\"\"\"\n",
        "    train_state_initializer = t5x.utils.TrainStateInitializer(\n",
        "      optimizer_def=self.model.optimizer_def,\n",
        "      init_fn=self.model.get_initial_variables,\n",
        "      input_shapes=self.input_shapes,\n",
        "      partitioner=self.partitioner)\n",
        "    restore_checkpoint_cfg = t5x.utils.RestoreCheckpointConfig(\n",
        "        path=checkpoint_path, mode='specific', dtype='float32')\n",
        "    train_state_axes = train_state_initializer.train_state_axes\n",
        "    self._predict_fn = self._get_predict_fn(train_state_axes)\n",
        "    self._train_state = train_state_initializer.from_checkpoint_or_scratch(\n",
        "        [restore_checkpoint_cfg], init_rng=jax.random.PRNGKey(0))\n",
        "  @functools.lru_cache()\n",
        "  def _get_predict_fn(self, train_state_axes):\n",
        "    \"\"\"Generate a partitioned prediction function for decoding.\"\"\"\n",
        "    def partial_predict_fn(params, batch, decode_rng):\n",
        "      return self.model.predict_batch_with_aux(\n",
        "          params, batch, decoder_params={'decode_rng': None})\n",
        "    return self.partitioner.partition(\n",
        "        partial_predict_fn,\n",
        "        in_axis_resources=(\n",
        "            train_state_axes.params,\n",
        "            t5x.partitioning.PartitionSpec('data',), None),\n",
        "        out_axis_resources=t5x.partitioning.PartitionSpec('data',)\n",
        "    )\n",
        "  def predict_tokens(self, batch, seed=0):\n",
        "    \"\"\"Predict tokens from preprocessed dataset batch.\"\"\"\n",
        "    prediction, _ = self._predict_fn(\n",
        "        self._train_state.params, batch, jax.random.PRNGKey(seed))\n",
        "    return self.vocabulary.decode_tf(prediction).numpy()\n",
        "  def __call__(self, audio):\n",
        "    \"\"\"Infer note sequence from audio samples.\n",
        "    Args:\n",
        "      audio: 1-d numpy array of audio samples (16kHz) for a single example.\n",
        "    Returns:\n",
        "      A note_sequence of the transcribed audio.\n",
        "    \"\"\"\n",
        "    ds = self.audio_to_dataset(audio)\n",
        "    ds = self.preprocess(ds)\n",
        "    model_ds = self.model.FEATURE_CONVERTER_CLS(pack=False)(\n",
        "        ds, task_feature_lengths=self.sequence_length)\n",
        "    model_ds = model_ds.batch(self.batch_size)\n",
        "    inferences = (tokens for batch in model_ds.as_numpy_iterator()\n",
        "                  for tokens in self.predict_tokens(batch))\n",
        "    predictions = []\n",
        "    for example, tokens in zip(ds.as_numpy_iterator(), inferences):\n",
        "      predictions.append(self.postprocess(tokens, example))\n",
        "    result = metrics_utils.event_predictions_to_ns(\n",
        "        predictions, codec=self.codec, encoding_spec=self.encoding_spec)\n",
        "    return result['est_ns']\n",
        "  def audio_to_dataset(self, audio):\n",
        "    \"\"\"Create a TF Dataset of spectrograms from input audio.\"\"\"\n",
        "    frames, frame_times = self._audio_to_frames(audio)\n",
        "    return tf.data.Dataset.from_tensors({\n",
        "        'inputs': frames,\n",
        "        'input_times': frame_times,\n",
        "    })\n",
        "  def _audio_to_frames(self, audio):\n",
        "    \"\"\"Compute spectrogram frames from audio.\"\"\"\n",
        "    frame_size = self.spectrogram_config.hop_width\n",
        "    padding = [0, frame_size - len(audio) % frame_size]\n",
        "    audio = np.pad(audio, padding, mode='constant')\n",
        "    frames = spectrograms.split_audio(audio, self.spectrogram_config)\n",
        "    num_frames = len(audio) // frame_size\n",
        "    times = np.arange(num_frames) / self.spectrogram_config.frames_per_second\n",
        "    return frames, times\n",
        "  def preprocess(self, ds):\n",
        "    pp_chain = [\n",
        "        functools.partial(\n",
        "            t5.data.preprocessors.split_tokens_to_inputs_length,\n",
        "            sequence_length=self.sequence_length,\n",
        "            output_features=self.output_features,\n",
        "            feature_key='inputs',\n",
        "            additional_feature_keys=['input_times']),\n",
        "        preprocessors.add_dummy_targets,\n",
        "        functools.partial(\n",
        "            preprocessors.compute_spectrograms,\n",
        "            spectrogram_config=self.spectrogram_config)\n",
        "    ]\n",
        "    for pp in pp_chain:\n",
        "      ds = pp(ds)\n",
        "    return ds\n",
        "  def postprocess(self, tokens, example):\n",
        "    tokens = self._trim_eos(tokens)\n",
        "    start_time = example['input_times'][0]\n",
        "    start_time -= start_time % (1 / self.codec.steps_per_second)\n",
        "    return {\n",
        "        'est_tokens': tokens,\n",
        "        'start_time': start_time,\n",
        "        'raw_inputs': []\n",
        "    }\n",
        "  @staticmethod\n",
        "  def _trim_eos(tokens):\n",
        "    tokens = np.array(tokens, np.int32)\n",
        "    if vocabularies.DECODED_EOS_ID in tokens:\n",
        "      tokens = tokens[:np.argmax(tokens == vocabularies.DECODED_EOS_ID)]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGQ-zpgy3raf"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL = \"mt3\"\n",
        "checkpoint_path = '/content/checkpoints/mt3/'\n",
        "\n",
        "def SetupTranscriptionModel(checkpoint_path, model_type='mt3'):\n",
        "    \"\"\"Set up the transcription model with checkpoint.\"\"\"\n",
        "    AnalyticsSetup()\n",
        "    LogAnalyticsEvent('loadModelStart', {'event_category': model_type})\n",
        "    config = initialize_transcription_config(model_type=model_type)\n",
        "    parse_gin_config(['/content/mt3/gin/model.gin', f'/content/mt3/gin/{model_type}.gin'],\n",
        "                     config['codec'].vocab_config.num_velocity_bins)\n",
        "    model = BuildTranscriptionModel(config)\n",
        "    predict_fn, train_state = restore_model_from_checkpoint(model, checkpoint_path, config)\n",
        "    LogAnalyticsEvent('loadModelComplete', {'event_category': model_type})\n",
        "    return config, model, predict_fn, train_state\n",
        "\n",
        "\n",
        "load_gtag()\n",
        "log_event('loadModelStart', {'event_category': MODEL})\n",
        "inference_model = InferenceModel(checkpoint_path, MODEL)\n",
        "log_event('loadModelComplete', {'event_category': MODEL})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2s3JAvBAgFB"
      },
      "outputs": [],
      "source": [
        "def UploadAndProcessAudio(sample_rate=16000):\n",
        "    \"\"\"Upload and process audio file with logging.\"\"\"\n",
        "    AnalyticsSetup()\n",
        "    LogAnalyticsEvent('uploadAudioStart', {})\n",
        "    audio = upload_audio_file(sample_rate)\n",
        "    LogAnalyticsEvent('uploadAudioComplete', {'value': round(len(audio) / sample_rate)})\n",
        "    return audio\n",
        "\n",
        "def Note_seq_play(audio, sample_rate=16000):\n",
        "    \"\"\"Play audio using note sequence utilities.\"\"\"\n",
        "    note_seq.notebook_utils.colab_play(audio, sample_rate=sample_rate)\n",
        "\n",
        "\n",
        "load_gtag()\n",
        "log_event('uploadAudioStart', {})\n",
        "audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
        "log_event('uploadAudioComplete', {'value': round(len(audio) / SAMPLE_RATE)})\n",
        "note_seq.notebook_utils.colab_play(audio, sample_rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSKNjUYYv1kV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile\n",
        "import gin\n",
        "import os\n",
        "import t5x\n",
        "import jax\n",
        "import tensorflow.compat.v2 as tf\n",
        "import functools\n",
        "from mt3 import network, spectrograms, preprocessors\n",
        "\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 229\n",
        "MEL_FMIN = 30\n",
        "MEL_FMAX = SAMPLE_RATE // 2\n",
        "WINDOW_LENGTH = 2048\n",
        "\n",
        "\n",
        "def preprocess_audio(audio_path):\n",
        "    \"\"\"Load and validate audio (aligned with dataset.py).\"\"\"\n",
        "    try:\n",
        "        audio, sr = soundfile.read(audio_path, dtype='int16')\n",
        "        assert sr == SAMPLE_RATE\n",
        "        if audio.ndim == 2:\n",
        "            audio = np.mean(audio, axis=1)\n",
        "        audio = audio / np.max(np.abs(audio))\n",
        "        audio = torch.ShortTensor(audio * 32768.0)\n",
        "        return audio, len(audio), True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio {audio_path}: {e}\")\n",
        "        return None, 0, False\n",
        "\n",
        "def parse_gin_config(gin_files, num_velocity_bins):\n",
        "    \"\"\"Parse gin configuration files with num_velocity_bins.\"\"\"\n",
        "    gin_bindings = [\n",
        "        'from __gin__ import dynamic_registration',\n",
        "        'from mt3 import vocabularies',\n",
        "        'VOCAB_CONFIG=@vocabularies.VocabularyConfig()',\n",
        "        f'vocabularies.VocabularyConfig.num_velocity_bins={num_velocity_bins}'\n",
        "    ]\n",
        "    with gin.unlock_config():\n",
        "        for gin_file in gin_files:\n",
        "            if not os.path.exists(gin_file):\n",
        "                raise FileNotFoundError(f\"Gin file not found: {gin_file}\")\n",
        "        gin.parse_config_files_and_bindings(gin_files, gin_bindings, finalize_config=False)\n",
        "\n",
        "def preprocess_audio_to_dataset(audio, spectrogram_config):\n",
        "    \"\"\"Convert audio to dataset with spectrogram frames.\"\"\"\n",
        "    frame_size = spectrogram_config.hop_width\n",
        "    padding = [0, frame_size - len(audio) % frame_size]\n",
        "    audio = np.pad(audio, padding, mode='constant')\n",
        "    frames = spectrograms.split_audio(audio, spectrogram_config)\n",
        "\n",
        "    if frames.ndim == 1:\n",
        "        frames = np.expand_dims(frames, axis=-1)\n",
        "    frames = np.expand_dims(frames, axis=0)\n",
        "    if frames.shape[-1] == 1:\n",
        "        frames = np.repeat(frames, N_MELS, axis=-1)\n",
        "    num_frames = len(audio) // frame_size\n",
        "    times = np.arange(num_frames) / spectrogram_config.frames_per_second\n",
        "    print(f\"preprocess_audio_to_dataset: frames shape = {frames.shape}\")\n",
        "    return tf.data.Dataset.from_tensors({\n",
        "        'inputs': frames,\n",
        "        'input_times': times\n",
        "    })\n",
        "\n",
        "def preprocess_dataset(ds, config):\n",
        "    \"\"\"Preprocess dataset with spectrogram computation.\"\"\"\n",
        "    pp_chain = [\n",
        "        functools.partial(\n",
        "            t5.data.preprocessors.split_tokens_to_inputs_length,\n",
        "            sequence_length=config['sequence_length'],\n",
        "            output_features=config['output_features'],\n",
        "            feature_key='inputs',\n",
        "            additional_feature_keys=['input_times']\n",
        "        ),\n",
        "        preprocessors.add_dummy_targets,\n",
        "        functools.partial(\n",
        "            preprocessors.compute_spectrograms,\n",
        "            spectrogram_config=config['spectrogram_config']\n",
        "        )\n",
        "    ]\n",
        "    for pp in pp_chain:\n",
        "        ds = pp(ds)\n",
        "\n",
        "    def ensure_3d_inputs(example):\n",
        "        inputs = example['inputs']\n",
        "        if inputs.ndim == 2:\n",
        "            inputs = tf.expand_dims(inputs, axis=-1)\n",
        "\n",
        "        if inputs.shape[-1] != N_MELS:\n",
        "            inputs = tf.image.resize(inputs[..., tf.newaxis],\n",
        "                                   [inputs.shape[1], N_MELS],\n",
        "                                   method='bilinear')[..., 0]\n",
        "        example['inputs'] = inputs\n",
        "        print(f\"preprocess_dataset: inputs shape = {inputs.shape}\")\n",
        "        return example\n",
        "    ds = ds.map(ensure_3d_inputs)\n",
        "    return ds\n",
        "\n",
        "def BuildTranscriptionModel(config):\n",
        "    \"\"\"Build the transcription model using a compatible T5X model.\"\"\"\n",
        "    model_config = gin.get_configurable(network.T5Config)()\n",
        "    module = network.Transformer(config=model_config)\n",
        "    try:\n",
        "        return t5x.models.ContinuousInputsEncoderDecoderModel(\n",
        "            module=module,\n",
        "            input_vocabulary=config['output_features']['inputs'].vocabulary,\n",
        "            output_vocabulary=config['output_features']['targets'].vocabulary,\n",
        "            optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
        "            input_depth=config['spectrogram_config'].input_depth\n",
        "        )\n",
        "    except AttributeError:\n",
        "        print(\"Warning: ContinuousInputsEncoderDecoderModel not found. Using EncoderDecoderModel.\")\n",
        "        return t5x.models.EncoderDecoderModel(\n",
        "            module=module,\n",
        "            input_vocabulary=config['output_features']['inputs'].vocabulary,\n",
        "            output_vocabulary=config['output_features']['targets'].vocabulary,\n",
        "            optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0)\n",
        "        )\n",
        "\n",
        "def SetupTranscriptionModel(checkpoint_path, model_type='mt3'):\n",
        "    \"\"\"Set up the transcription model with checkpoint.\"\"\"\n",
        "    AnalyticsSetup()\n",
        "    LogAnalyticsEvent('loadModelStart', {'event_category': model_type})\n",
        "\n",
        "    config = initialize_transcription_config(model_type=model_type)\n",
        "    num_velocity_bins = 1 if model_type == 'mt3' else 127\n",
        "    gin_files = ['/content/mt3/gin/model.gin', f'/content/mt3/gin/{model_type}.gin']\n",
        "    parse_gin_config(gin_files, num_velocity_bins)\n",
        "    model = BuildTranscriptionModel(config)\n",
        "    predict_fn, train_state = restore_model_from_checkpoint(model, checkpoint_path, config)\n",
        "    LogAnalyticsEvent('loadModelComplete', {'event_category': model_type})\n",
        "    return config, model, predict_fn, train_state\n",
        "\n",
        "def TranscribeAudio(audio, config, predict_fn, train_state, model_type='mt3'):\n",
        "    \"\"\"Transcribe audio to note sequence with logging.\"\"\"\n",
        "    AnalyticsSetup()\n",
        "    LogAnalyticsEvent('transcribeStart', {\n",
        "        'event_category': model_type,\n",
        "        'value': round(len(audio) / 16000)\n",
        "    })\n",
        "    est_ns = transcribe_audio(audio, predict_fn, train_state, config)\n",
        "    LogAnalyticsEvent('transcribeComplete', {\n",
        "        'event_category': model_type,\n",
        "        'value': round(len(audio) / 16000),\n",
        "        'numNotes': sum(1 for note in est_ns.notes if not note.is_drum),\n",
        "        'numDrumNotes': sum(1 for note in est_ns.notes if note.is_drum),\n",
        "        'numPrograms': len(set(note.program for note in est_ns.notes if not note.is_drum))\n",
        "    })\n",
        "    return est_ns\n",
        "\n",
        "def Note_seq_play_sequence(note_sequence, sample_rate=16000, sf2_path='SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'):\n",
        "    \"\"\"Play transcribed note sequence.\"\"\"\n",
        "    note_seq.play_sequence(note_sequence, synth=note_seq.fluidsynth, sample_rate=sample_rate, sf2_path=sf2_path)\n",
        "\n",
        "def Note_seq_visualize(note_sequence):\n",
        "    \"\"\"Visualize transcribed note sequence.\"\"\"\n",
        "    note_seq.plot_sequence(note_sequence)\n",
        "\n",
        "\n",
        "\n",
        "def main_transcription(audio, config, predict_fn, train_state, model_type='mt3', sample_rate=16000, sf2_path='SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'):\n",
        "    \"\"\"Main function to transcribe audio, display mel spectrogram, TSV, and play/visualize sequence.\"\"\"\n",
        "    est_ns = TranscribeAudio(audio, config, predict_fn, train_state, model_type)\n",
        "    DisplayMelSpectrogram(audio, sample_rate)\n",
        "    DisplayTSVContent(est_ns)\n",
        "    Note_seq_play_sequence(est_ns, sample_rate, sf2_path)\n",
        "    Note_seq_visualize(est_ns)\n",
        "    return est_ns\n",
        "\n",
        "\n",
        "load_gtag()\n",
        "log_event('transcribeStart', {\n",
        "    'event_category': MODEL,\n",
        "    'value': round(len(audio) / SAMPLE_RATE)\n",
        "})\n",
        "est_ns = inference_model(audio)\n",
        "log_event('transcribeComplete', {\n",
        "    'event_category': MODEL,\n",
        "    'value': round(len(audio) / SAMPLE_RATE),\n",
        "    'numNotes': sum(1 for note in est_ns.notes if not note.is_drum),\n",
        "    'numDrumNotes': sum(1 for note in est_ns.notes if note.is_drum),\n",
        "    'numPrograms': len(set(note.program for note in est_ns.notes\n",
        "                           if not note.is_drum))\n",
        "})\n",
        "note_seq.play_sequence(est_ns, synth=note_seq.fluidsynth,\n",
        "                       sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
        "note_seq.plot_sequence(est_ns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DiCjtDpyUMh"
      },
      "outputs": [],
      "source": [
        "def Note_seq_to_midi(note_sequence, output_path='/tmp/transcribed.mid'):\n",
        "    \"\"\"Export note sequence to MIDI file with logging.\"\"\"\n",
        "    AnalyticsSetup()\n",
        "    LogAnalyticsEvent('downloadTranscription', {\n",
        "        'event_category': 'mt3',\n",
        "        'value': round(len(audio) / SAMPLE_RATE),\n",
        "        'numNotes': sum(1 for note in note_sequence.notes if not note.is_drum),\n",
        "        'numDrumNotes': sum(1 for note in note_sequence.notes if note.is_drum),\n",
        "        'numPrograms': len(set(note.program for note in note_sequence.notes if not note.is_drum))\n",
        "    })\n",
        "    note_seq.sequence_proto_to_midi_file(note_sequence, output_path)\n",
        "\n",
        "def PrintHumanReadableMidi(note_sequence, time_step=0.1):\n",
        "    \"\"\"Print MIDI notes in human-readable format.\"\"\"\n",
        "    if not note_sequence.notes:\n",
        "        print(\"No notes in the sequence.\")\n",
        "        return\n",
        "    max_time = max(note.end_time for note in note_sequence.notes)\n",
        "    time_bins = np.arange(0, max_time + time_step, time_step)\n",
        "    midi_to_note = {0: 'C', 1: 'C#', 2: 'D', 3: 'D#', 4: 'E', 5: 'F',\n",
        "                    6: 'F#', 7: 'G', 8: 'G#', 9: 'A', 10: 'A#', 11: 'B'}\n",
        "\n",
        "    for t in time_bins:\n",
        "        active_notes = []\n",
        "        for note in note_sequence.notes:\n",
        "            if note.start_time <= t < note.end_time and not note.is_drum:\n",
        "                octave = (note.pitch // 12) - 1\n",
        "                note_name = midi_to_note[note.pitch % 12]\n",
        "                active_notes.append(f\"{note_name}{octave}\")\n",
        "        if active_notes:\n",
        "            print(f\"At time {t:.2f}s: {' '.join(active_notes)}\")\n",
        "\n",
        "\n",
        "load_gtag()\n",
        "log_event('downloadTranscription', {\n",
        "    'event_category': MODEL,\n",
        "    'value': round(len(audio) / SAMPLE_RATE),\n",
        "    'numNotes': sum(1 for note in est_ns.notes if not note.is_drum),\n",
        "    'numDrumNotes': sum(1 for note in est_ns.notes if note.is_drum),\n",
        "    'numPrograms': len(set(note.program for note in est_ns.notes\n",
        "                           if not note.is_drum))\n",
        "})\n",
        "note_seq.sequence_proto_to_midi_file(est_ns, '/tmp/transcribed.mid')\n",
        "\n",
        "\n",
        "PrintHumanReadableMidi(est_ns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter, metadata, environment\n",
        "from IPython.display import Image, display\n",
        "import music21\n",
        "\n",
        "def install_lilypond():\n",
        "    \"\"\"Install Lilypond for music sheet generation.\"\"\"\n",
        "    os.system(\"apt-get update\")\n",
        "    os.system(\"apt-get install lilypond -y\")\n",
        "\n",
        "def configure_music21_environment():\n",
        "    \"\"\"Configure music21 environment for Lilypond.\"\"\"\n",
        "    env = environment.Environment()\n",
        "    env['musicxmlPath'] = '/usr/bin/lilypond'\n",
        "    env['lilypondPath'] = '/usr/bin/lilypond'\n",
        "    return env\n",
        "\n",
        "def Music21_convert(midi_path, title=\"Music Sheet Created with Love @SoC\"):\n",
        "    \"\"\"Convert MIDI file to music sheet.\"\"\"\n",
        "    score = converter.parse(midi_path)\n",
        "    score.metadata = metadata.Metadata()\n",
        "    score.metadata.title = title\n",
        "    return score\n",
        "\n",
        "def DisplayMusicSheet(score, output_format='lily.png'):\n",
        "    \"\"\"Display music sheet from score.\"\"\"\n",
        "    score.write(output_format)\n",
        "    display(Image(filename=score.write(output_format)))\n",
        "\n",
        "\n",
        "from music21 import converter, metadata\n",
        "from IPython.display import Image, display\n",
        "from music21 import environment\n",
        "env = environment.Environment()\n",
        "env['musicxmlPath'] = '/usr/bin/lilypond'\n",
        "env['lilypondPath'] = '/usr/bin/lilypond'\n",
        "midi_file_path = '/tmp/transcribed.mid'\n",
        "score = converter.parse(midi_file_path)\n",
        "score.metadata = metadata.Metadata()\n",
        "score.metadata.title = \"Music Sheet Generated with Love @SoC\"\n",
        "score.show('lily.png')\n",
        "display(Image(filename=score.write('lily.png')))\n",
        "\n"
      ],
      "metadata": {
        "id": "ieI9O_bBDy-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIQEG8tzEA-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}