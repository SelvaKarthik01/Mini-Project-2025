{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11483231,"sourceType":"datasetVersion","datasetId":7197166}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tqdm import tqdm\nimport logging\n\n# Suppress TensorFlow CUDA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Constants\nSR = 16000\nN_MELS = 229\nHOP_LENGTH = 512\nTARGET_FRAMES = 480\nNOTES = 88\nINPUT_DIR = \"/kaggle/input\"  # Base input directory\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"preprocessed\", \"preprocessed\")  # Point to the inner preprocessed directory\nWORKING_DIR = \"/kaggle/working\"  # Where to save models\nBATCH_SIZE = 8\nITERATIONS = 2400  # Adjusted to target ~50 epochs with 120 batches/epoch\nCHECKPOINT_INTERVAL = 240  # Save checkpoint every 240 steps\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify directory structure with debugging\ndef verify_data_dirs():\n    expected_dirs = [\n        os.path.join(OUTPUT_DIR, 'train', 'mel'),\n        os.path.join(OUTPUT_DIR, 'train', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'validation', 'mel'),\n        os.path.join(OUTPUT_DIR, 'validation', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'test', 'mel'),\n        os.path.join(OUTPUT_DIR, 'test', 'piano_roll')\n    ]\n    for dir_path in expected_dirs:\n        if not os.path.exists(dir_path):\n            logger.error(f\"Directory {dir_path} not found.\")\n            logger.error(f\"Contents of {os.path.dirname(dir_path)}: {os.listdir(os.path.dirname(dir_path))}\")\n            raise FileNotFoundError(f\"Directory {dir_path} not found. Check dataset structure in {OUTPUT_DIR}.\")\n        if not os.listdir(dir_path):\n            raise FileNotFoundError(f\"Directory {dir_path} is empty. Check dataset contents.\")\n\n# Normalize adjacency matrix\ndef normalize_adj(A, symmetric=True):\n    A = A + tf.eye(tf.shape(A)[0], dtype=tf.float32)\n    d = tf.reduce_sum(A, axis=1)\n    if symmetric:\n        D_inv_sqrt = tf.linalg.diag(tf.pow(d, -0.5))\n        return D_inv_sqrt @ A @ D_inv_sqrt\n    else:\n        D_inv = tf.linalg.diag(tf.pow(d, -1))\n        return D_inv @ A\n\n# Compute adjacency matrix\ndef compute_adjacency_matrix(train_roll_dir):\n    train_rolls = []\n    for f in os.listdir(train_roll_dir):\n        if f.endswith('.npy'):\n            roll = np.load(os.path.join(train_roll_dir, f))\n            train_rolls.append(roll)\n    \n    all_frames = np.concatenate(train_rolls, axis=1)\n    all_frames = (all_frames > 0).astype(np.float32)\n    \n    M = tf.matmul(all_frames, all_frames, transpose_b=True)\n    M = M + tf.transpose(M)\n    M_max = tf.reduce_max(M)\n    P = M / M_max if M_max > 0 else M\n    A = tf.cast(P >= 0.6, tf.float32)\n    A = tf.cast((A + tf.transpose(A)) > 0, tf.float32)\n    A = normalize_adj(A, symmetric=True)\n    \n    return A\n\n# GCN Layer\nclass GCNSimple(tf.keras.layers.Layer):\n    def __init__(self, dim_in, dim_out):\n        super(GCNSimple, self).__init__()\n        train_roll_dir = os.path.join(OUTPUT_DIR, 'train', 'piano_roll')\n        self.A = tf.constant(compute_adjacency_matrix(train_roll_dir), dtype=tf.float32)\n        self.fc1 = layers.Dense(dim_in, use_bias=False)\n        self.fc2 = layers.Dense(dim_out // 2, use_bias=False)\n        self.fc3 = layers.Dense(dim_out, use_bias=False)\n    \n    def call(self, inputs):\n        X = tf.nn.relu(self.fc1(self.A))\n        X = tf.nn.relu(self.fc2(tf.matmul(self.A, X)))\n        X = self.fc3(tf.matmul(self.A, X))\n        return tf.matmul(inputs, X)\n\n# ConvStack\ndef build_conv_stack(input_features, output_features):\n    model = models.Sequential([\n        layers.Input(shape=(None, input_features, 1)),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.25),\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.25),\n        layers.Reshape((-1, (output_features // 8) * (input_features // 4))),\n        layers.Dense(output_features),\n        layers.Dropout(0.5)\n    ])\n    return model\n\n# OnsetsAndFrames model\nclass OnsetsAndFrames(Model):\n    def __init__(self, input_features, output_features, model_complexity=48):\n        super(OnsetsAndFrames, self).__init__()\n        model_size = model_complexity * 16\n        \n        self.onset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.offset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.frame_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.combined_stack = models.Sequential([\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True), \n                               input_shape=(None, output_features * 3)),\n            layers.Dense(output_features),\n            GCNSimple(output_features, output_features),\n            layers.Activation('sigmoid')\n        ])\n    \n    def call(self, mel, training=False):\n        mel = tf.expand_dims(mel, -1)\n        onset_pred = self.onset_stack(mel, training=training)\n        offset_pred = self.offset_stack(mel, training=training)\n        activation_pred = self.frame_stack(mel, training=training)\n        combined_pred = tf.concat([onset_pred, offset_pred, activation_pred], axis=-1)\n        frame_pred = self.combined_stack(combined_pred, training=training)\n        return {\n            'onset': onset_pred,\n            'offset': offset_pred,\n            'frame': frame_pred\n        }\n\n# Data generator\nclass PianoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, split, batch_size=8):\n        self.split = split\n        self.batch_size = batch_size\n        self.audio_dir = os.path.join(OUTPUT_DIR, split, 'mel')\n        self.roll_dir = os.path.join(OUTPUT_DIR, split, 'piano_roll')\n        self.audio_files = [f for f in os.listdir(self.audio_dir) if f.endswith('.npy')]\n        self.roll_files = [f for f in os.listdir(self.roll_dir) if f.endswith('.npy')]\n        self.audio_files.sort()\n        self.roll_files.sort()\n        assert len(self.audio_files) == len(self.roll_files), f\"Mismatch in {split}\"\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.audio_files) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_audio_files = self.audio_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_roll_files = self.roll_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.array([np.load(os.path.join(self.audio_dir, f)) for f in batch_audio_files])\n        y = np.array([np.load(os.path.join(self.roll_dir, f)) for f in batch_roll_files])\n        \n        # Ensure X is 3D (time, features) or reshape if necessary\n        if len(X.shape) == 2:\n            X = X[:, :, np.newaxis]  # Add channel dimension if missing\n        elif len(X.shape) == 3 and X.shape[1] != N_MELS:\n            X = np.transpose(X, (0, 2, 1))  # Transpose to (batch, time, features) if needed\n        else:\n            X = np.transpose(X, (0, 2, 1))  # Standard transpose\n        \n        # Ensure y is 3D (time, notes)\n        if len(y.shape) == 2:\n            y = y[:, :, np.newaxis]\n        elif len(y.shape) == 3 and y.shape[1] != NOTES:\n            y = np.transpose(y, (0, 2, 1))\n        else:\n            y = np.transpose(y, (0, 2, 1))\n        \n        return X, {'onset': y, 'offset': y, 'frame': y}\n    \n    def on_epoch_end(self):\n        indices = np.arange(len(self.audio_files))\n        np.random.shuffle(indices)\n        self.audio_files = [self.audio_files[i] for i in indices]\n        self.roll_files = [self.roll_files[i] for i in indices]\n\n# Custom training step with loss only\nclass CustomModel(Model):\n    def __init__(self, onsets_and_frames_model):\n        super(CustomModel, self).__init__()\n        self.onsets_and_frames = onsets_and_frames_model\n        self.clip_norm = 3.0\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.onset_loss_tracker = tf.keras.metrics.Mean(name=\"onset_loss\")\n        self.offset_loss_tracker = tf.keras.metrics.Mean(name=\"offset_loss\")\n        self.frame_loss_tracker = tf.keras.metrics.Mean(name=\"frame_loss\")\n    \n    def call(self, inputs, training=False):\n        return self.onsets_and_frames(inputs, training=training)\n    \n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        losses = {}\n        for key in ['onset', 'offset', 'frame']:\n            losses[key] = tf.keras.losses.binary_crossentropy(y[key], y_pred[key])\n        total_loss = sum(losses.values()) / len(losses)\n        return total_loss, losses['onset'], losses['offset'], losses['frame']\n    \n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(total_loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, self.clip_norm) if g is not None else g for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        # Update metrics\n        self.loss_tracker.update_state(total_loss)\n        self.onset_loss_tracker.update_state(onset_loss)\n        self.offset_loss_tracker.update_state(offset_loss)\n        self.frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.loss_tracker.result(),\n            \"onset_loss\": self.onset_loss_tracker.result(),\n            \"offset_loss\": self.offset_loss_tracker.result(),\n            \"frame_loss\": self.frame_loss_tracker.result()\n        }\n    \n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.onset_loss_tracker, self.offset_loss_tracker, self.frame_loss_tracker]\n\n# Training function\ndef train_model():\n    # Initialize data generators\n    train_gen = PianoDataGenerator('train', batch_size=BATCH_SIZE)\n    \n    # Estimate epochs to reach 2400 iterations\n    batches_per_epoch = len(train_gen)\n    total_epochs = 50  # Fixed to 50 epochs as requested\n    logger.info(f\"Training for {total_epochs} epochs with {batches_per_epoch} batches/epoch\")\n    \n    # Initialize model\n    onsets_and_frames = OnsetsAndFrames(input_features=N_MELS, output_features=NOTES)\n    model = CustomModel(onsets_and_frames)\n    \n    # Learning rate schedule\n    initial_learning_rate = 0.0006\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=10000 // batches_per_epoch,\n        decay_rate=0.98,\n        staircase=True\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=None,\n        metrics=None\n    )\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        os.path.join(WORKING_DIR, \"best_model.keras\"),\n        monitor=\"loss\",\n        save_best_only=True,\n        verbose=1\n    )\n    \n    # Custom callback with checkpointing only (no validation)\n    class CustomCallback(tf.keras.callbacks.Callback):\n        def __init__(self, checkpoint_interval):\n            super().__init__()\n            self.checkpoint_interval = checkpoint_interval\n            self.global_step = 0\n        \n        def on_batch_end(self, batch, logs=None):\n            self.global_step += 1\n            if self.global_step % self.checkpoint_interval == 0:\n                self.model.save(os.path.join(WORKING_DIR, f\"model_step_{self.global_step}.keras\"))\n                logger.info(f\"Step {self.global_step}: Saved checkpoint\")\n    \n    # Train without validation\n    history = model.fit(\n        train_gen,\n        epochs=total_epochs,\n        callbacks=[\n            checkpoint,\n            CustomCallback(CHECKPOINT_INTERVAL)\n        ],\n        verbose=1\n    )\n    \n    return model, history\n\nif __name__ == '__main__':\n    # Verify data directories\n    verify_data_dirs()\n    \n    # Start training\n    model, history = train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:00:09.146105Z","iopub.execute_input":"2025-04-20T09:00:09.146904Z","iopub.status.idle":"2025-04-20T10:54:09.152227Z","shell.execute_reply.started":"2025-04-20T09:00:09.146879Z","shell.execute_reply":"2025-04-20T10:54:09.151699Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745139633.177163      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/custom_model_2_1/onsets_and_frames_2_1/sequential_15_1/sequential_14_1/dropout_18_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.3513 - loss: 0.2491 - offset_loss: 0.2001 - onset_loss: 0.1959\nEpoch 1: loss improved from inf to 0.17921, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - frame_loss: 0.3503 - loss: 0.2485 - offset_loss: 0.1997 - onset_loss: 0.1955\nEpoch 2/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1561 - loss: 0.1225 - offset_loss: 0.1097 - onset_loss: 0.1017\nEpoch 2: loss improved from 0.17921 to 0.11523, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.1560 - loss: 0.1224 - offset_loss: 0.1097 - onset_loss: 0.1016\nEpoch 3/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1356 - loss: 0.0971 - offset_loss: 0.0807 - onset_loss: 0.0749\nEpoch 3: loss improved from 0.11523 to 0.09511, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.1355 - loss: 0.0970 - offset_loss: 0.0807 - onset_loss: 0.0749\nEpoch 4/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1163 - loss: 0.0825 - offset_loss: 0.0674 - onset_loss: 0.0639\nEpoch 4: loss improved from 0.09511 to 0.08265, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.1163 - loss: 0.0825 - offset_loss: 0.0674 - onset_loss: 0.0639\nEpoch 5/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1023 - loss: 0.0742 - offset_loss: 0.0621 - onset_loss: 0.0582\nEpoch 5: loss improved from 0.08265 to 0.07255, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.1023 - loss: 0.0742 - offset_loss: 0.0621 - onset_loss: 0.0582\nEpoch 6/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0909 - loss: 0.0673 - offset_loss: 0.0567 - onset_loss: 0.0544\nEpoch 6: loss improved from 0.07255 to 0.06405, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0909 - loss: 0.0673 - offset_loss: 0.0567 - onset_loss: 0.0543\nEpoch 7/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0800 - loss: 0.0603 - offset_loss: 0.0512 - onset_loss: 0.0496\nEpoch 7: loss improved from 0.06405 to 0.05929, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0800 - loss: 0.0603 - offset_loss: 0.0512 - onset_loss: 0.0496\nEpoch 8/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0706 - loss: 0.0542 - offset_loss: 0.0471 - onset_loss: 0.0449\nEpoch 8: loss improved from 0.05929 to 0.05410, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0706 - loss: 0.0542 - offset_loss: 0.0471 - onset_loss: 0.0449\nEpoch 9/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0647 - loss: 0.0506 - offset_loss: 0.0446 - onset_loss: 0.0424\nEpoch 9: loss improved from 0.05410 to 0.04988, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0647 - loss: 0.0506 - offset_loss: 0.0446 - onset_loss: 0.0424\nEpoch 10/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0597 - loss: 0.0473 - offset_loss: 0.0417 - onset_loss: 0.0404\nEpoch 10: loss improved from 0.04988 to 0.04660, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0597 - loss: 0.0472 - offset_loss: 0.0417 - onset_loss: 0.0404\nEpoch 11/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0542 - loss: 0.0441 - offset_loss: 0.0396 - onset_loss: 0.0383\nEpoch 11: loss improved from 0.04660 to 0.04351, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0542 - loss: 0.0441 - offset_loss: 0.0396 - onset_loss: 0.0383\nEpoch 12/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0494 - loss: 0.0408 - offset_loss: 0.0372 - onset_loss: 0.0358\nEpoch 12: loss improved from 0.04351 to 0.04070, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0494 - loss: 0.0408 - offset_loss: 0.0372 - onset_loss: 0.0358\nEpoch 13/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0468 - loss: 0.0398 - offset_loss: 0.0369 - onset_loss: 0.0356\nEpoch 13: loss improved from 0.04070 to 0.03802, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0467 - loss: 0.0397 - offset_loss: 0.0369 - onset_loss: 0.0355\nEpoch 14/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0411 - loss: 0.0355 - offset_loss: 0.0333 - onset_loss: 0.0319\nEpoch 14: loss improved from 0.03802 to 0.03591, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0411 - loss: 0.0355 - offset_loss: 0.0333 - onset_loss: 0.0320\nEpoch 15/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0394 - loss: 0.0345 - offset_loss: 0.0327 - onset_loss: 0.0316\nEpoch 15: loss improved from 0.03591 to 0.03463, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0394 - loss: 0.0345 - offset_loss: 0.0327 - onset_loss: 0.0316\nEpoch 16/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0360 - loss: 0.0322 - offset_loss: 0.0310 - onset_loss: 0.0295\nEpoch 16: loss improved from 0.03463 to 0.03275, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0360 - loss: 0.0322 - offset_loss: 0.0310 - onset_loss: 0.0295\nEpoch 17/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0352 - loss: 0.0317 - offset_loss: 0.0307 - onset_loss: 0.0293\nEpoch 17: loss improved from 0.03275 to 0.03169, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0352 - loss: 0.0317 - offset_loss: 0.0307 - onset_loss: 0.0293\nEpoch 18/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0323 - loss: 0.0294 - offset_loss: 0.0285 - onset_loss: 0.0274\nEpoch 18: loss improved from 0.03169 to 0.03030, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0323 - loss: 0.0294 - offset_loss: 0.0285 - onset_loss: 0.0274\nEpoch 19/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0315 - loss: 0.0291 - offset_loss: 0.0285 - onset_loss: 0.0272\nEpoch 19: loss improved from 0.03030 to 0.02922, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0315 - loss: 0.0291 - offset_loss: 0.0285 - onset_loss: 0.0272\nEpoch 20/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0290 - loss: 0.0272 - offset_loss: 0.0270 - onset_loss: 0.0255\nEpoch 20: loss improved from 0.02922 to 0.02824, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0290 - loss: 0.0272 - offset_loss: 0.0270 - onset_loss: 0.0255\nEpoch 21/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0281 - loss: 0.0264 - offset_loss: 0.0261 - onset_loss: 0.0250\nEpoch 21: loss improved from 0.02824 to 0.02709, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0281 - loss: 0.0264 - offset_loss: 0.0261 - onset_loss: 0.0251\nEpoch 22/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0270 - loss: 0.0257 - offset_loss: 0.0255 - onset_loss: 0.0246\nEpoch 22: loss improved from 0.02709 to 0.02592, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0270 - loss: 0.0257 - offset_loss: 0.0255 - onset_loss: 0.0246\nEpoch 23/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0265 - loss: 0.0253 - offset_loss: 0.0251 - onset_loss: 0.0242\nEpoch 23: loss improved from 0.02592 to 0.02529, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0265 - loss: 0.0253 - offset_loss: 0.0251 - onset_loss: 0.0242\nEpoch 24/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0241 - loss: 0.0234 - offset_loss: 0.0235 - onset_loss: 0.0225\nEpoch 24: loss improved from 0.02529 to 0.02453, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0242 - loss: 0.0234 - offset_loss: 0.0235 - onset_loss: 0.0225\nEpoch 25/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0245 - loss: 0.0237 - offset_loss: 0.0237 - onset_loss: 0.0229\nEpoch 25: loss improved from 0.02453 to 0.02382, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0245 - loss: 0.0237 - offset_loss: 0.0237 - onset_loss: 0.0229\nEpoch 26/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0228 - loss: 0.0225 - offset_loss: 0.0227 - onset_loss: 0.0219\nEpoch 26: loss improved from 0.02382 to 0.02298, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0228 - loss: 0.0225 - offset_loss: 0.0227 - onset_loss: 0.0219\nEpoch 27/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0223 - loss: 0.0219 - offset_loss: 0.0221 - onset_loss: 0.0213\nEpoch 27: loss improved from 0.02298 to 0.02250, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0223 - loss: 0.0219 - offset_loss: 0.0221 - onset_loss: 0.0213\nEpoch 28/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0197 - loss: 0.0197 - offset_loss: 0.0199 - onset_loss: 0.0195\nEpoch 28: loss improved from 0.02250 to 0.02174, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0197 - loss: 0.0197 - offset_loss: 0.0200 - onset_loss: 0.0195\nEpoch 29/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0207 - loss: 0.0206 - offset_loss: 0.0209 - onset_loss: 0.0203\nEpoch 29: loss improved from 0.02174 to 0.02120, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0207 - loss: 0.0206 - offset_loss: 0.0209 - onset_loss: 0.0203\nEpoch 30/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0196 - loss: 0.0199 - offset_loss: 0.0203 - onset_loss: 0.0197\nEpoch 30: loss improved from 0.02120 to 0.02035, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0196 - loss: 0.0199 - offset_loss: 0.0203 - onset_loss: 0.0197\nEpoch 31/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0196 - loss: 0.0197 - offset_loss: 0.0200 - onset_loss: 0.0195\nEpoch 31: loss improved from 0.02035 to 0.01979, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0196 - loss: 0.0197 - offset_loss: 0.0200 - onset_loss: 0.0195\nEpoch 32/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0184 - loss: 0.0188 - offset_loss: 0.0192 - onset_loss: 0.0188\nEpoch 32: loss improved from 0.01979 to 0.01916, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0184 - loss: 0.0188 - offset_loss: 0.0192 - onset_loss: 0.0188\nEpoch 33/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0173 - loss: 0.0178 - offset_loss: 0.0183 - onset_loss: 0.0177\nEpoch 33: loss improved from 0.01916 to 0.01880, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0173 - loss: 0.0178 - offset_loss: 0.0183 - onset_loss: 0.0177\nEpoch 34/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0181 - loss: 0.0186 - offset_loss: 0.0190 - onset_loss: 0.0186\nEpoch 34: loss improved from 0.01880 to 0.01821, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0181 - loss: 0.0186 - offset_loss: 0.0190 - onset_loss: 0.0186\nEpoch 35/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0173 - loss: 0.0180 - offset_loss: 0.0185 - onset_loss: 0.0181\nEpoch 35: loss improved from 0.01821 to 0.01773, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0173 - loss: 0.0180 - offset_loss: 0.0185 - onset_loss: 0.0181\nEpoch 36/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0163 - loss: 0.0171 - offset_loss: 0.0177 - onset_loss: 0.0173\nEpoch 36: loss improved from 0.01773 to 0.01732, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0163 - loss: 0.0171 - offset_loss: 0.0177 - onset_loss: 0.0173\nEpoch 37/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0156 - loss: 0.0164 - offset_loss: 0.0170 - onset_loss: 0.0167\nEpoch 37: loss improved from 0.01732 to 0.01670, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0156 - loss: 0.0164 - offset_loss: 0.0170 - onset_loss: 0.0167\nEpoch 38/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0152 - loss: 0.0162 - offset_loss: 0.0168 - onset_loss: 0.0167\nEpoch 38: loss improved from 0.01670 to 0.01636, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0152 - loss: 0.0162 - offset_loss: 0.0168 - onset_loss: 0.0167\nEpoch 39/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0160 - loss: 0.0169 - offset_loss: 0.0174 - onset_loss: 0.0173\nEpoch 39: loss improved from 0.01636 to 0.01601, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0160 - loss: 0.0169 - offset_loss: 0.0174 - onset_loss: 0.0172\nEpoch 40/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0144 - loss: 0.0154 - offset_loss: 0.0160 - onset_loss: 0.0159\nEpoch 40: loss improved from 0.01601 to 0.01557, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0144 - loss: 0.0154 - offset_loss: 0.0160 - onset_loss: 0.0159\nEpoch 41/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0140 - loss: 0.0152 - offset_loss: 0.0159 - onset_loss: 0.0158\nEpoch 41: loss improved from 0.01557 to 0.01521, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0140 - loss: 0.0152 - offset_loss: 0.0159 - onset_loss: 0.0158\nEpoch 42/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0130 - loss: 0.0144 - offset_loss: 0.0151 - onset_loss: 0.0150\nEpoch 42: loss improved from 0.01521 to 0.01475, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0131 - loss: 0.0144 - offset_loss: 0.0151 - onset_loss: 0.0150\nEpoch 43/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0126 - loss: 0.0140 - offset_loss: 0.0146 - onset_loss: 0.0147\nEpoch 43: loss improved from 0.01475 to 0.01445, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0126 - loss: 0.0140 - offset_loss: 0.0146 - onset_loss: 0.0147\nEpoch 44/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0127 - loss: 0.0140 - offset_loss: 0.0145 - onset_loss: 0.0146\nEpoch 44: loss improved from 0.01445 to 0.01412, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0127 - loss: 0.0140 - offset_loss: 0.0145 - onset_loss: 0.0146\nEpoch 45/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0123 - loss: 0.0138 - offset_loss: 0.0144 - onset_loss: 0.0145\nEpoch 45: loss improved from 0.01412 to 0.01379, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0124 - loss: 0.0138 - offset_loss: 0.0144 - onset_loss: 0.0145\nEpoch 46/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0117 - loss: 0.0131 - offset_loss: 0.0138 - onset_loss: 0.0138\nEpoch 46: loss improved from 0.01379 to 0.01343, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0117 - loss: 0.0131 - offset_loss: 0.0138 - onset_loss: 0.0138\nEpoch 47/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0119 - loss: 0.0133 - offset_loss: 0.0139 - onset_loss: 0.0141\nEpoch 47: loss improved from 0.01343 to 0.01321, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0119 - loss: 0.0133 - offset_loss: 0.0139 - onset_loss: 0.0141\nEpoch 48/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0113 - loss: 0.0127 - offset_loss: 0.0134 - onset_loss: 0.0135\nEpoch 48: loss improved from 0.01321 to 0.01297, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0113 - loss: 0.0127 - offset_loss: 0.0134 - onset_loss: 0.0135\nEpoch 49/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0113 - loss: 0.0127 - offset_loss: 0.0134 - onset_loss: 0.0136\nEpoch 49: loss improved from 0.01297 to 0.01257, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0113 - loss: 0.0127 - offset_loss: 0.0134 - onset_loss: 0.0136\nEpoch 50/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0105 - loss: 0.0121 - offset_loss: 0.0127 - onset_loss: 0.0129\nEpoch 50: loss improved from 0.01257 to 0.01232, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0105 - loss: 0.0121 - offset_loss: 0.0127 - onset_loss: 0.0129\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport logging\n\n# Suppress TensorFlow CUDA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Constants\nSR = 16000\nN_MELS = 229\nHOP_LENGTH = 512\nTARGET_FRAMES = 480\nNOTES = 88\nINPUT_DIR = \"/kaggle/input\"  # Base input directory\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"preprocessed\", \"preprocessed\")  # Point to the inner preprocessed directory\nWORKING_DIR = \"/kaggle/working\"  # Where to save models\nBATCH_SIZE = 8\nITERATIONS = 2400  # Adjusted to target ~50 epochs with 120 batches/epoch\nCHECKPOINT_INTERVAL = 240  # Save checkpoint every 240 steps\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify directory structure with debugging\ndef verify_data_dirs():\n    expected_dirs = [\n        os.path.join(OUTPUT_DIR, 'train', 'mel'),\n        os.path.join(OUTPUT_DIR, 'train', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'validation', 'mel'),\n        os.path.join(OUTPUT_DIR, 'validation', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'test', 'mel'),\n        os.path.join(OUTPUT_DIR, 'test', 'piano_roll')\n    ]\n    for dir_path in expected_dirs:\n        if not os.path.exists(dir_path):\n            logger.error(f\"Directory {dir_path} not found.\")\n            logger.error(f\"Contents of {os.path.dirname(dir_path)}: {os.listdir(os.path.dirname(dir_path))}\")\n            raise FileNotFoundError(f\"Directory {dir_path} not found. Check dataset structure in {OUTPUT_DIR}.\")\n        if not os.listdir(dir_path):\n            raise FileNotFoundError(f\"Directory {dir_path} is empty. Check dataset contents.\")\n\n# Normalize adjacency matrix\ndef normalize_adj(A, symmetric=True):\n    A = A + tf.eye(tf.shape(A)[0], dtype=tf.float32)\n    d = tf.reduce_sum(A, axis=1)\n    if symmetric:\n        D_inv_sqrt = tf.linalg.diag(tf.pow(d, -0.5))\n        return D_inv_sqrt @ A @ D_inv_sqrt\n    else:\n        D_inv = tf.linalg.diag(tf.pow(d, -1))\n        return D_inv @ A\n\n# Compute adjacency matrix\ndef compute_adjacency_matrix(train_roll_dir):\n    train_rolls = []\n    for f in os.listdir(train_roll_dir):\n        if f.endswith('.npy'):\n            roll = np.load(os.path.join(train_roll_dir, f))\n            train_rolls.append(roll)\n    \n    all_frames = np.concatenate(train_rolls, axis=1)\n    all_frames = (all_frames > 0).astype(np.float32)\n    \n    M = tf.matmul(all_frames, all_frames, transpose_b=True)\n    M = M + tf.transpose(M)\n    M_max = tf.reduce_max(M)\n    P = M / M_max if M_max > 0 else M\n    A = tf.cast(P >= 0.6, tf.float32)\n    A = tf.cast((A + tf.transpose(A)) > 0, tf.float32)\n    A = normalize_adj(A, symmetric=True)\n    \n    return A\n\n# GCN Layer\nclass GCNSimple(tf.keras.layers.Layer):\n    def __init__(self, dim_in, dim_out, **kwargs):\n        super(GCNSimple, self).__init__(**kwargs)\n        train_roll_dir = os.path.join(OUTPUT_DIR, 'train', 'piano_roll')\n        self.A = tf.constant(compute_adjacency_matrix(train_roll_dir), dtype=tf.float32)\n        self.fc1 = layers.Dense(dim_in, use_bias=False)\n        self.fc2 = layers.Dense(dim_out // 2, use_bias=False)\n        self.fc3 = layers.Dense(dim_out, use_bias=False)\n    \n    def call(self, inputs):\n        X = tf.nn.relu(self.fc1(self.A))\n        X = tf.nn.relu(self.fc2(tf.matmul(self.A, X)))\n        X = self.fc3(tf.matmul(self.A, X))\n        return tf.matmul(inputs, X)\n    \n    def get_config(self):\n        config = super(GCNSimple, self).get_config()\n        config.update({'dim_in': self.fc1.units, 'dim_out': self.fc3.units})\n        return config\n\n# ConvStack\ndef build_conv_stack(input_features, output_features):\n    model = models.Sequential([\n        layers.Input(shape=(None, input_features, 1)),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.2),\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.2),\n        layers.Reshape((-1, (output_features // 8) * (input_features // 4))),\n        layers.Dense(output_features),\n        layers.Dropout(0.5)\n    ])\n    return model\n\n# OnsetsAndFrames model\nclass OnsetsAndFrames(Model):\n    def __init__(self, input_features, output_features, model_complexity=48, **kwargs):\n        super(OnsetsAndFrames, self).__init__(**kwargs)\n        self.input_features = input_features\n        self.output_features = output_features\n        self.model_complexity = model_complexity\n        model_size = model_complexity * 16\n        \n        self.onset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.offset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.frame_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.combined_stack = models.Sequential([\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True), \n                               input_shape=(None, output_features * 3)),\n            layers.Dense(output_features),\n            GCNSimple(output_features, output_features),\n            layers.Activation('sigmoid')\n        ])\n    \n    def call(self, mel, training=False):\n        mel = tf.expand_dims(mel, -1)\n        onset_pred = self.onset_stack(mel, training=training)\n        offset_pred = self.offset_stack(mel, training=training)\n        activation_pred = self.frame_stack(mel, training=training)\n        combined_pred = tf.concat([onset_pred, offset_pred, activation_pred], axis=-1)\n        frame_pred = self.combined_stack(combined_pred, training=training)\n        return {'onset': onset_pred, 'offset': offset_pred, 'frame': frame_pred}\n    \n    def get_config(self):\n        config = super(OnsetsAndFrames, self).get_config()\n        config.update({\n            'input_features': self.input_features,\n            'output_features': self.output_features,\n            'model_complexity': self.model_complexity\n        })\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        input_features = config.get('input_features', N_MELS)\n        output_features = config.get('output_features', NOTES)\n        model_complexity = config.get('model_complexity', 48)\n        return cls(input_features=input_features, \n                   output_features=output_features, \n                   model_complexity=model_complexity)\n\n# Data generator\nclass PianoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, split, batch_size=8, shuffle=False):\n        self.split = split\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.audio_dir = os.path.join(OUTPUT_DIR, split, 'mel')\n        self.roll_dir = os.path.join(OUTPUT_DIR, split, 'piano_roll')\n        self.audio_files = [f for f in os.listdir(self.audio_dir) if f.endswith('.npy')]\n        self.roll_files = [f for f in os.listdir(self.roll_dir) if f.endswith('.npy')]\n        self.audio_files.sort()\n        self.roll_files.sort()\n        assert len(self.audio_files) == len(self.roll_files), f\"Mismatch in {split}\"\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.audio_files) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_audio_files = self.audio_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_roll_files = self.roll_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.array([np.load(os.path.join(self.audio_dir, f)) for f in batch_audio_files])\n        y = np.array([np.load(os.path.join(self.roll_dir, f)) for f in batch_roll_files])\n        \n        # Ensure 3D shape\n        if X.ndim == 2:\n            X = np.expand_dims(X, axis=0)\n        if y.ndim == 2:\n            y = np.expand_dims(y, axis=0)\n        \n        # Transpose to (batch, frames, features)\n        if X.shape[1] == N_MELS and X.shape[2] == TARGET_FRAMES:\n            X = np.transpose(X, (0, 2, 1))\n        elif X.shape[1] != TARGET_FRAMES or X.shape[2] != N_MELS:\n            raise ValueError(f\"Unexpected X shape: {X.shape}\")\n        \n        if y.shape[1] == NOTES and y.shape[2] == TARGET_FRAMES:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.shape[1] != TARGET_FRAMES or y.shape[2] != NOTES:\n            raise ValueError(f\"Unexpected y shape: {y.shape}\")\n        \n        # Return y as a dictionary to match model expectations\n        return X, {'onset': y, 'offset': y, 'frame': y}\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            indices = np.arange(len(self.audio_files))\n            np.random.shuffle(indices)\n            self.audio_files = [self.audio_files[i] for i in indices]\n            self.roll_files = [self.roll_files[i] for i in indices]\n\n# Custom training step with loss only\nclass CustomModel(Model):\n    def __init__(self, onsets_and_frames_model):\n        super(CustomModel, self).__init__()\n        self.onsets_and_frames = onsets_and_frames_model\n        self.clip_norm = 3.0\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.onset_loss_tracker = tf.keras.metrics.Mean(name=\"onset_loss\")\n        self.offset_loss_tracker = tf.keras.metrics.Mean(name=\"offset_loss\")\n        self.frame_loss_tracker = tf.keras.metrics.Mean(name=\"frame_loss\")\n    \n    def call(self, inputs, training=False):\n        return self.onsets_and_frames(inputs, training=training)\n    \n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        losses = {}\n        for key in ['onset', 'offset', 'frame']:\n            losses[key] = tf.keras.losses.binary_crossentropy(y[key], y_pred[key])\n        total_loss = sum(losses.values()) / len(losses)\n        return total_loss, losses['onset'], losses['offset'], losses['frame']\n    \n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(total_loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, self.clip_norm) if g is not None else g for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        # Update metrics\n        self.loss_tracker.update_state(total_loss)\n        self.onset_loss_tracker.update_state(onset_loss)\n        self.offset_loss_tracker.update_state(offset_loss)\n        self.frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.loss_tracker.result(),\n            \"onset_loss\": self.onset_loss_tracker.result(),\n            \"offset_loss\": self.offset_loss_tracker.result(),\n            \"frame_loss\": self.frame_loss_tracker.result()\n        }\n    \n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.onset_loss_tracker, self.offset_loss_tracker, self.frame_loss_tracker]\n\n# Training function\ndef train_model():\n    # Initialize data generators\n    train_gen = PianoDataGenerator('train', batch_size=BATCH_SIZE)\n    \n    # Estimate epochs to reach 2400 iterations\n    batches_per_epoch = len(train_gen)\n    total_epochs = 50  # Fixed to 50 epochs as requested\n    logger.info(f\"Training for {total_epochs} epochs with {batches_per_epoch} batches/epoch\")\n    \n    # Initialize model\n    onsets_and_frames = OnsetsAndFrames(input_features=N_MELS, output_features=NOTES)\n    model = CustomModel(onsets_and_frames)\n    \n    # Learning rate schedule\n    initial_learning_rate = 0.0006\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=10000 // batches_per_epoch,\n        decay_rate=0.98,\n        staircase=True\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=None,\n        metrics=None\n    )\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        os.path.join(WORKING_DIR, \"best_model.keras\"),\n        monitor=\"loss\",\n        save_best_only=True,\n        verbose=1\n    )\n    \n    # Custom callback with checkpointing only (no validation)\n    class CustomCallback(tf.keras.callbacks.Callback):\n        def __init__(self, checkpoint_interval):\n            super().__init__()\n            self.checkpoint_interval = checkpoint_interval\n            self.global_step = 0\n        \n        def on_batch_end(self, batch, logs=None):\n            self.global_step += 1\n            if self.global_step % self.checkpoint_interval == 0:\n                self.model.save(os.path.join(WORKING_DIR, f\"model_step_{self.global_step}.keras\"))\n                logger.info(f\"Step {self.global_step}: Saved checkpoint\")\n    \n    # Train without validation\n    history = model.fit(\n        train_gen,\n        epochs=total_epochs,\n        callbacks=[\n            checkpoint,\n            CustomCallback(CHECKPOINT_INTERVAL)\n        ],\n        verbose=1\n    )\n    \n    return model, history\n\n# Evaluation function\ndef evaluate_model(model, test_gen):\n    max_samples = min(10, len(test_gen))\n    Y_true_all, Y_pred_all = [], []\n    \n    for i in range(max_samples):\n        try:\n            batch_x, batch_y = test_gen[i]\n            batch_y = batch_y['frame']  # Use 'frame' key for evaluation\n        except ValueError as e:\n            print(f\"⏭️ Skipping batch {i}: {e}\")\n            continue\n        \n        preds = model.predict(batch_x, verbose=0)\n        pred_frame = preds['frame']\n        pred_frame = np.round(pred_frame).astype(int)\n        \n        Y_pred_all.append(pred_frame[0] if BATCH_SIZE == 1 else pred_frame)\n        Y_true_all.append(batch_y[0] if BATCH_SIZE == 1 else batch_y)\n    \n    Y_true_flat = np.concatenate(Y_true_all).reshape(-1)\n    Y_pred_flat = np.concatenate(Y_pred_all).reshape(-1)\n    \n    frame_precision = precision_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_recall = recall_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_f1 = f1_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    \n    print(\"\\n🎯 Frame-level Metrics:\")\n    print(f\"Precision: {frame_precision:.4f}\")\n    print(f\"Recall: {frame_recall:.4f}\")\n    print(f\"F1-score: {frame_f1:.4f}\")\n    \n    if frame_f1 >= 0.9277:\n        print(\"\\nFrame F1 Score exceeds CR-GCN's 92.77% - Excellent!\")\n    else:\n        print(f\"\\nFrame F1 Score is {frame_f1:.4f}, below CR-GCN's 92.77%. Consider further optimization.\")\n    \n    return frame_f1\n\nif __name__ == '__main__':\n    # Verify data directories\n    verify_data_dirs()\n    \n    # Train the model\n    model, history = train_model()\n    \n    # Evaluate the model\n    test_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE, shuffle=False)\n    evaluate_model(model, test_gen)\n    \n    # Save the final model (optional, in case best_model.keras wasn't updated)\n    model.save(os.path.join(WORKING_DIR, \"final_model.keras\"))\n    logger.info(f\"Saved final model to {os.path.join(WORKING_DIR, 'final_model.keras')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:21:01.933956Z","iopub.execute_input":"2025-04-20T11:21:01.934800Z","iopub.status.idle":"2025-04-20T13:20:01.598667Z","shell.execute_reply.started":"2025-04-20T11:21:01.934775Z","shell.execute_reply":"2025-04-20T13:20:01.598094Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745148084.814517      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/custom_model_1_1/onsets_and_frames_1_1/sequential_8_1/sequential_7_1/dropout_9_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nI0000 00:00:1745148087.533353     107 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.3866 - loss: 0.2604 - offset_loss: 0.1961 - onset_loss: 0.1984\nEpoch 1: loss improved from inf to 0.17919, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 1s/step - frame_loss: 0.3854 - loss: 0.2597 - offset_loss: 0.1957 - onset_loss: 0.1979\nEpoch 2/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1506 - loss: 0.1124 - offset_loss: 0.0932 - onset_loss: 0.0936\nEpoch 2: loss improved from 0.17919 to 0.11017, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.1506 - loss: 0.1124 - offset_loss: 0.0931 - onset_loss: 0.0935\nEpoch 3/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1380 - loss: 0.0957 - offset_loss: 0.0743 - onset_loss: 0.0748\nEpoch 3: loss improved from 0.11017 to 0.09310, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.1380 - loss: 0.0957 - offset_loss: 0.0743 - onset_loss: 0.0748\nEpoch 4/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1158 - loss: 0.0808 - offset_loss: 0.0632 - onset_loss: 0.0634\nEpoch 4: loss improved from 0.09310 to 0.07845, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.1158 - loss: 0.0808 - offset_loss: 0.0632 - onset_loss: 0.0634\nEpoch 5/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0967 - loss: 0.0688 - offset_loss: 0.0544 - onset_loss: 0.0553\nEpoch 5: loss improved from 0.07845 to 0.06951, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0967 - loss: 0.0688 - offset_loss: 0.0544 - onset_loss: 0.0553\nEpoch 6/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0847 - loss: 0.0621 - offset_loss: 0.0507 - onset_loss: 0.0510\nEpoch 6: loss improved from 0.06951 to 0.06114, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0847 - loss: 0.0621 - offset_loss: 0.0507 - onset_loss: 0.0510\nEpoch 7/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0753 - loss: 0.0563 - offset_loss: 0.0466 - onset_loss: 0.0469\nEpoch 7: loss improved from 0.06114 to 0.05504, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0753 - loss: 0.0563 - offset_loss: 0.0466 - onset_loss: 0.0469\nEpoch 8/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0675 - loss: 0.0521 - offset_loss: 0.0443 - onset_loss: 0.0444\nEpoch 8: loss improved from 0.05504 to 0.05007, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0675 - loss: 0.0520 - offset_loss: 0.0442 - onset_loss: 0.0444\nEpoch 9/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0595 - loss: 0.0468 - offset_loss: 0.0405 - onset_loss: 0.0404\nEpoch 9: loss improved from 0.05007 to 0.04713, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0595 - loss: 0.0468 - offset_loss: 0.0405 - onset_loss: 0.0404\nEpoch 10/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0523 - loss: 0.0421 - offset_loss: 0.0371 - onset_loss: 0.0370\nEpoch 10: loss improved from 0.04713 to 0.04338, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0523 - loss: 0.0421 - offset_loss: 0.0371 - onset_loss: 0.0370\nEpoch 11/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0484 - loss: 0.0399 - offset_loss: 0.0355 - onset_loss: 0.0357\nEpoch 11: loss improved from 0.04338 to 0.04051, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0484 - loss: 0.0399 - offset_loss: 0.0355 - onset_loss: 0.0357\nEpoch 12/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0464 - loss: 0.0388 - offset_loss: 0.0351 - onset_loss: 0.0349\nEpoch 12: loss improved from 0.04051 to 0.03848, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0464 - loss: 0.0388 - offset_loss: 0.0351 - onset_loss: 0.0349\nEpoch 13/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0390 - loss: 0.0337 - offset_loss: 0.0313 - onset_loss: 0.0310\nEpoch 14: loss improved from 0.03591 to 0.03415, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0390 - loss: 0.0337 - offset_loss: 0.0313 - onset_loss: 0.0310\nEpoch 15/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0377 - loss: 0.0331 - offset_loss: 0.0310 - onset_loss: 0.0308\nEpoch 15: loss improved from 0.03415 to 0.03253, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0377 - loss: 0.0331 - offset_loss: 0.0310 - onset_loss: 0.0308\nEpoch 16/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0346 - loss: 0.0309 - offset_loss: 0.0292 - onset_loss: 0.0288\nEpoch 16: loss improved from 0.03253 to 0.03121, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0346 - loss: 0.0309 - offset_loss: 0.0292 - onset_loss: 0.0288\nEpoch 17/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0339 - loss: 0.0304 - offset_loss: 0.0287 - onset_loss: 0.0287\nEpoch 17: loss improved from 0.03121 to 0.02986, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0339 - loss: 0.0304 - offset_loss: 0.0287 - onset_loss: 0.0286\nEpoch 18/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0315 - loss: 0.0287 - offset_loss: 0.0273 - onset_loss: 0.0272\nEpoch 18: loss improved from 0.02986 to 0.02860, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0315 - loss: 0.0287 - offset_loss: 0.0273 - onset_loss: 0.0272\nEpoch 19/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0293 - loss: 0.0269 - offset_loss: 0.0258 - onset_loss: 0.0256\nEpoch 19: loss improved from 0.02860 to 0.02742, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0293 - loss: 0.0269 - offset_loss: 0.0258 - onset_loss: 0.0256\nEpoch 20/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0273 - loss: 0.0254 - offset_loss: 0.0246 - onset_loss: 0.0242\nEpoch 20: loss improved from 0.02742 to 0.02634, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0273 - loss: 0.0254 - offset_loss: 0.0246 - onset_loss: 0.0242\nEpoch 21/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0275 - loss: 0.0258 - offset_loss: 0.0252 - onset_loss: 0.0247\nEpoch 21: loss improved from 0.02634 to 0.02531, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0275 - loss: 0.0258 - offset_loss: 0.0252 - onset_loss: 0.0247\nEpoch 22/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0251 - loss: 0.0237 - offset_loss: 0.0232 - onset_loss: 0.0229\nEpoch 22: loss improved from 0.02531 to 0.02430, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0251 - loss: 0.0237 - offset_loss: 0.0232 - onset_loss: 0.0229\nEpoch 23/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0226 - loss: 0.0218 - offset_loss: 0.0215 - onset_loss: 0.0213\nEpoch 23: loss improved from 0.02430 to 0.02341, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0226 - loss: 0.0218 - offset_loss: 0.0215 - onset_loss: 0.0213\nEpoch 24/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0246 - loss: 0.0236 - offset_loss: 0.0232 - onset_loss: 0.0230\nEpoch 24: loss improved from 0.02341 to 0.02295, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0246 - loss: 0.0236 - offset_loss: 0.0232 - onset_loss: 0.0230\nEpoch 25/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0234 - loss: 0.0225 - offset_loss: 0.0221 - onset_loss: 0.0219\nEpoch 25: loss improved from 0.02295 to 0.02215, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0234 - loss: 0.0225 - offset_loss: 0.0221 - onset_loss: 0.0219\nEpoch 26/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0212 - loss: 0.0208 - offset_loss: 0.0206 - onset_loss: 0.0205\nEpoch 26: loss improved from 0.02215 to 0.02124, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0212 - loss: 0.0208 - offset_loss: 0.0206 - onset_loss: 0.0205\nEpoch 27/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0211 - loss: 0.0207 - offset_loss: 0.0207 - onset_loss: 0.0203\nEpoch 27: loss improved from 0.02124 to 0.02041, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0211 - loss: 0.0207 - offset_loss: 0.0207 - onset_loss: 0.0203\nEpoch 28/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0201 - loss: 0.0199 - offset_loss: 0.0199 - onset_loss: 0.0198\nEpoch 28: loss improved from 0.02041 to 0.01977, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0201 - loss: 0.0199 - offset_loss: 0.0199 - onset_loss: 0.0198\nEpoch 29/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0184 - loss: 0.0185 - offset_loss: 0.0187 - onset_loss: 0.0185\nEpoch 29: loss improved from 0.01977 to 0.01917, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0184 - loss: 0.0185 - offset_loss: 0.0187 - onset_loss: 0.0185\nEpoch 30/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0176 - loss: 0.0179 - offset_loss: 0.0180 - onset_loss: 0.0179\nEpoch 30: loss improved from 0.01917 to 0.01856, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0176 - loss: 0.0179 - offset_loss: 0.0180 - onset_loss: 0.0179\nEpoch 31/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0178 - loss: 0.0184 - offset_loss: 0.0188 - onset_loss: 0.0186\nEpoch 31: loss improved from 0.01856 to 0.01805, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0178 - loss: 0.0184 - offset_loss: 0.0188 - onset_loss: 0.0186\nEpoch 32/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0171 - loss: 0.0176 - offset_loss: 0.0180 - onset_loss: 0.0177\nEpoch 32: loss improved from 0.01805 to 0.01732, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0171 - loss: 0.0176 - offset_loss: 0.0180 - onset_loss: 0.0177\nEpoch 33/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0166 - loss: 0.0172 - offset_loss: 0.0178 - onset_loss: 0.0172\nEpoch 33: loss improved from 0.01732 to 0.01680, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0166 - loss: 0.0172 - offset_loss: 0.0178 - onset_loss: 0.0172\nEpoch 34/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0155 - loss: 0.0161 - offset_loss: 0.0166 - onset_loss: 0.0163\nEpoch 34: loss improved from 0.01680 to 0.01626, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0155 - loss: 0.0161 - offset_loss: 0.0166 - onset_loss: 0.0163\nEpoch 35/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0157 - loss: 0.0164 - offset_loss: 0.0170 - onset_loss: 0.0165\nEpoch 35: loss improved from 0.01626 to 0.01584, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0157 - loss: 0.0164 - offset_loss: 0.0170 - onset_loss: 0.0165\nEpoch 36/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0150 - loss: 0.0160 - offset_loss: 0.0167 - onset_loss: 0.0165\nEpoch 36: loss improved from 0.01584 to 0.01537, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0150 - loss: 0.0160 - offset_loss: 0.0167 - onset_loss: 0.0164\nEpoch 37/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0134 - loss: 0.0145 - offset_loss: 0.0151 - onset_loss: 0.0148\nEpoch 37: loss improved from 0.01537 to 0.01486, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0134 - loss: 0.0145 - offset_loss: 0.0151 - onset_loss: 0.0148\nEpoch 38/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0135 - loss: 0.0146 - offset_loss: 0.0153 - onset_loss: 0.0149\nEpoch 38: loss improved from 0.01486 to 0.01446, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0135 - loss: 0.0146 - offset_loss: 0.0153 - onset_loss: 0.0149\nEpoch 39/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0121 - loss: 0.0133 - offset_loss: 0.0141 - onset_loss: 0.0138\nEpoch 39: loss improved from 0.01446 to 0.01406, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0122 - loss: 0.0133 - offset_loss: 0.0141 - onset_loss: 0.0138\nEpoch 40/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0124 - loss: 0.0135 - offset_loss: 0.0143 - onset_loss: 0.0139\nEpoch 40: loss improved from 0.01406 to 0.01359, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0124 - loss: 0.0135 - offset_loss: 0.0143 - onset_loss: 0.0139\nEpoch 41/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0125 - loss: 0.0137 - offset_loss: 0.0145 - onset_loss: 0.0142\nEpoch 41: loss improved from 0.01359 to 0.01326, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0124 - loss: 0.0137 - offset_loss: 0.0145 - onset_loss: 0.0142\nEpoch 42/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0115 - loss: 0.0128 - offset_loss: 0.0135 - onset_loss: 0.0132\nEpoch 42: loss improved from 0.01326 to 0.01284, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - frame_loss: 0.0115 - loss: 0.0128 - offset_loss: 0.0135 - onset_loss: 0.0132\nEpoch 43/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0117 - loss: 0.0131 - offset_loss: 0.0139 - onset_loss: 0.0136\nEpoch 43: loss improved from 0.01284 to 0.01258, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0117 - loss: 0.0131 - offset_loss: 0.0139 - onset_loss: 0.0136\nEpoch 44/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0108 - loss: 0.0122 - offset_loss: 0.0130 - onset_loss: 0.0128\nEpoch 44: loss improved from 0.01258 to 0.01224, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0108 - loss: 0.0122 - offset_loss: 0.0130 - onset_loss: 0.0128\nEpoch 45/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0100 - loss: 0.0113 - offset_loss: 0.0122 - onset_loss: 0.0119\nEpoch 45: loss improved from 0.01224 to 0.01188, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0100 - loss: 0.0113 - offset_loss: 0.0122 - onset_loss: 0.0119\nEpoch 46/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0103 - loss: 0.0119 - offset_loss: 0.0128 - onset_loss: 0.0125\nEpoch 46: loss improved from 0.01188 to 0.01159, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0103 - loss: 0.0118 - offset_loss: 0.0128 - onset_loss: 0.0124\nEpoch 47/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0098 - loss: 0.0112 - offset_loss: 0.0121 - onset_loss: 0.0118\nEpoch 47: loss improved from 0.01159 to 0.01130, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0098 - loss: 0.0112 - offset_loss: 0.0121 - onset_loss: 0.0118\nEpoch 48/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0095 - loss: 0.0110 - offset_loss: 0.0119 - onset_loss: 0.0117\nEpoch 48: loss improved from 0.01130 to 0.01112, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0095 - loss: 0.0111 - offset_loss: 0.0119 - onset_loss: 0.0117\nEpoch 49/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0091 - loss: 0.0106 - offset_loss: 0.0114 - onset_loss: 0.0113\nEpoch 49: loss improved from 0.01112 to 0.01085, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - frame_loss: 0.0091 - loss: 0.0106 - offset_loss: 0.0114 - onset_loss: 0.0114\nEpoch 50/50\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0091 - loss: 0.0107 - offset_loss: 0.0116 - onset_loss: 0.0114\nEpoch 50: loss improved from 0.01085 to 0.01059, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - frame_loss: 0.0091 - loss: 0.0107 - offset_loss: 0.0116 - onset_loss: 0.0114\n\n🎯 Frame-level Metrics:\nPrecision: 0.7999\nRecall: 0.7116\nF1-score: 0.7532\n\nFrame F1 Score is 0.7532, below CR-GCN's 92.77%. Consider further optimization.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport logging\nimport csv\nimport matplotlib.pyplot as plt\n\n# Suppress TensorFlow CUDA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Constants\nSR = 16000\nN_MELS = 229\nHOP_LENGTH = 512\nTARGET_FRAMES = 480\nNOTES = 88\nINPUT_DIR = \"/kaggle/input\"  # Base input directory\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"preprocessed\", \"preprocessed\")  # Point to the inner preprocessed directory\nWORKING_DIR = \"/kaggle/working\"  # Where to save models\nBATCH_SIZE = 8\nITERATIONS = 2400  # Adjusted to target ~50 epochs with 120 batches/epoch\nCHECKPOINT_INTERVAL = 240  # Save checkpoint every 240 steps\nCSV_FILE = os.path.join(WORKING_DIR, \"training_metrics.csv\")\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify directory structure with debugging\ndef verify_data_dirs():\n    expected_dirs = [\n        os.path.join(OUTPUT_DIR, 'train', 'mel'),\n        os.path.join(OUTPUT_DIR, 'train', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'validation', 'mel'),\n        os.path.join(OUTPUT_DIR, 'validation', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'test', 'mel'),\n        os.path.join(OUTPUT_DIR, 'test', 'piano_roll')\n    ]\n    for dir_path in expected_dirs:\n        if not os.path.exists(dir_path):\n            logger.error(f\"Directory {dir_path} not found.\")\n            logger.error(f\"Contents of {os.path.dirname(dir_path)}: {os.listdir(os.path.dirname(dir_path))}\")\n            raise FileNotFoundError(f\"Directory {dir_path} not found. Check dataset structure in {OUTPUT_DIR}.\")\n        if not os.listdir(dir_path):\n            raise FileNotFoundError(f\"Directory {dir_path} is empty. Check dataset contents.\")\n\n# Normalize adjacency matrix\ndef normalize_adj(A, symmetric=True):\n    A = A + tf.eye(tf.shape(A)[0], dtype=tf.float32)\n    d = tf.reduce_sum(A, axis=1)\n    if symmetric:\n        D_inv_sqrt = tf.linalg.diag(tf.pow(d, -0.5))\n        return D_inv_sqrt @ A @ D_inv_sqrt\n    else:\n        D_inv = tf.linalg.diag(tf.pow(d, -1))\n        return D_inv @ A\n\n# Compute adjacency matrix\ndef compute_adjacency_matrix(train_roll_dir):\n    train_rolls = []\n    for f in os.listdir(train_roll_dir):\n        if f.endswith('.npy'):\n            roll = np.load(os.path.join(train_roll_dir, f))\n            train_rolls.append(roll)\n    \n    all_frames = np.concatenate(train_rolls, axis=1)\n    all_frames = (all_frames > 0).astype(np.float32)\n    \n    M = tf.matmul(all_frames, all_frames, transpose_b=True)\n    M = M + tf.transpose(M)\n    M_max = tf.reduce_max(M)\n    P = M / M_max if M_max > 0 else M\n    A = tf.cast(P >= 0.6, tf.float32)\n    A = tf.cast((A + tf.transpose(A)) > 0, tf.float32)\n    A = normalize_adj(A, symmetric=True)\n    \n    return A\n\n# GCN Layer\nclass GCNSimple(tf.keras.layers.Layer):\n    def __init__(self, dim_in, dim_out, **kwargs):\n        super(GCNSimple, self).__init__(**kwargs)\n        train_roll_dir = os.path.join(OUTPUT_DIR, 'train', 'piano_roll')\n        self.A = tf.constant(compute_adjacency_matrix(train_roll_dir), dtype=tf.float32)\n        self.fc1 = layers.Dense(dim_in, use_bias=False)\n        self.fc2 = layers.Dense(dim_out // 2, use_bias=False)\n        self.fc3 = layers.Dense(dim_out, use_bias=False)\n    \n    def call(self, inputs):\n        X = tf.nn.relu(self.fc1(self.A))\n        X = tf.nn.relu(self.fc2(tf.matmul(self.A, X)))\n        X = self.fc3(tf.matmul(self.A, X))\n        return tf.matmul(inputs, X)\n    \n    def get_config(self):\n        config = super(GCNSimple, self).get_config()\n        config.update({'dim_in': self.fc1.units, 'dim_out': self.fc3.units})\n        return config\n\n# ConvStack\ndef build_conv_stack(input_features, output_features):\n    model = models.Sequential([\n        layers.Input(shape=(None, input_features, 1)),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.45),\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.45),\n        layers.Reshape((-1, (output_features // 8) * (input_features // 4))),\n        layers.Dense(output_features),\n        layers.Dropout(0.45)\n    ])\n    return model\n\n# OnsetsAndFrames model\nclass OnsetsAndFrames(Model):\n    def __init__(self, input_features, output_features, model_complexity=48, **kwargs):\n        super(OnsetsAndFrames, self).__init__(**kwargs)\n        self.input_features = input_features\n        self.output_features = output_features\n        self.model_complexity = model_complexity\n        model_size = model_complexity * 16\n        \n        self.onset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.offset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.frame_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.combined_stack = models.Sequential([\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True), \n                               input_shape=(None, output_features * 3)),\n            layers.Dense(output_features),\n            GCNSimple(output_features, output_features),\n            layers.Activation('sigmoid')\n        ])\n    \n    def call(self, mel, training=False):\n        mel = tf.expand_dims(mel, -1)\n        onset_pred = self.onset_stack(mel, training=training)\n        offset_pred = self.offset_stack(mel, training=training)\n        activation_pred = self.frame_stack(mel, training=training)\n        combined_pred = tf.concat([onset_pred, offset_pred, activation_pred], axis=-1)\n        frame_pred = self.combined_stack(combined_pred, training=training)\n        return {'onset': onset_pred, 'offset': offset_pred, 'frame': frame_pred}\n    \n    def get_config(self):\n        config = super(OnsetsAndFrames, self).get_config()\n        config.update({\n            'input_features': self.input_features,\n            'output_features': self.output_features,\n            'model_complexity': self.model_complexity\n        })\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        input_features = config.get('input_features', N_MELS)\n        output_features = config.get('output_features', NOTES)\n        model_complexity = config.get('model_complexity', 48)\n        return cls(input_features=input_features, \n                   output_features=output_features, \n                   model_complexity=model_complexity)\n\n# Data generator\nclass PianoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, split, batch_size=8, shuffle=False):\n        self.split = split\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.audio_dir = os.path.join(OUTPUT_DIR, split, 'mel')\n        self.roll_dir = os.path.join(OUTPUT_DIR, split, 'piano_roll')\n        self.audio_files = [f for f in os.listdir(self.audio_dir) if f.endswith('.npy')]\n        self.roll_files = [f for f in os.listdir(self.roll_dir) if f.endswith('.npy')]\n        self.audio_files.sort()\n        self.roll_files.sort()\n        assert len(self.audio_files) == len(self.roll_files), f\"Mismatch in {split}\"\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.audio_files) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_audio_files = self.audio_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_roll_files = self.roll_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.array([np.load(os.path.join(self.audio_dir, f)) for f in batch_audio_files])\n        y = np.array([np.load(os.path.join(self.roll_dir, f)) for f in batch_roll_files])\n        \n        # Ensure 3D shape\n        if X.ndim == 2:\n            X = np.expand_dims(X, axis=0)\n        if y.ndim == 2:\n            y = np.expand_dims(y, axis=0)\n        \n        # Normalize X to [0, 1]\n        X = (X - np.min(X)) / (np.max(X) - np.min(X) + 1e-8)  # Add small epsilon to avoid division by zero\n        \n        # Transpose to (batch, frames, features)\n        if X.shape[1] == N_MELS and X.shape[2] == TARGET_FRAMES:\n            X = np.transpose(X, (0, 2, 1))\n        elif X.shape[1] != TARGET_FRAMES or X.shape[2] != N_MELS:\n            raise ValueError(f\"Unexpected X shape: {X.shape}\")\n        \n        if y.shape[1] == NOTES and y.shape[2] == TARGET_FRAMES:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.shape[1] != TARGET_FRAMES or y.shape[2] != NOTES:\n            raise ValueError(f\"Unexpected y shape: {y.shape}\")\n        \n        # Log shapes for debugging\n        logger.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n        \n        # Return y as a dictionary to match model expectations\n        return X, {'onset': y, 'offset': y, 'frame': y}\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            indices = np.arange(len(self.audio_files))\n            np.random.shuffle(indices)\n            self.audio_files = [self.audio_files[i] for i in indices]\n            self.roll_files = [self.roll_files[i] for i in indices]\n\n# Custom training step with loss only\nclass CustomModel(Model):\n    def __init__(self, onsets_and_frames_model):\n        super(CustomModel, self).__init__()\n        self.onsets_and_frames = onsets_and_frames_model\n        self.clip_norm = 3.0\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.onset_loss_tracker = tf.keras.metrics.Mean(name=\"onset_loss\")\n        self.offset_loss_tracker = tf.keras.metrics.Mean(name=\"offset_loss\")\n        self.frame_loss_tracker = tf.keras.metrics.Mean(name=\"frame_loss\")\n        self.val_loss_tracker = tf.keras.metrics.Mean(name=\"val_loss\")\n        self.val_onset_loss_tracker = tf.keras.metrics.Mean(name=\"val_onset_loss\")\n        self.val_offset_loss_tracker = tf.keras.metrics.Mean(name=\"val_offset_loss\")\n        self.val_frame_loss_tracker = tf.keras.metrics.Mean(name=\"val_frame_loss\")\n    \n    def call(self, inputs, training=False):\n        return self.onsets_and_frames(inputs, training=training)\n    \n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        losses = {}\n        for key in ['onset', 'offset', 'frame']:\n            losses[key] = tf.keras.losses.binary_crossentropy(y[key], y_pred[key])\n        total_loss = sum(losses.values()) / len(losses)\n        return total_loss, losses['onset'], losses['offset'], losses['frame']\n    \n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(total_loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, self.clip_norm) if g is not None else g for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        # Update training metrics\n        self.loss_tracker.update_state(total_loss)\n        self.onset_loss_tracker.update_state(onset_loss)\n        self.offset_loss_tracker.update_state(offset_loss)\n        self.frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.loss_tracker.result(),\n            \"onset_loss\": self.onset_loss_tracker.result(),\n            \"offset_loss\": self.offset_loss_tracker.result(),\n            \"frame_loss\": self.frame_loss_tracker.result()\n        }\n    \n    def test_step(self, data):\n        x, y = data\n        y_pred = self(x, training=False)\n        total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        # Update validation metrics\n        self.val_loss_tracker.update_state(total_loss)\n        self.val_onset_loss_tracker.update_state(onset_loss)\n        self.val_offset_loss_tracker.update_state(offset_loss)\n        self.val_frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.val_loss_tracker.result(),  # Unprefixed names\n            \"onset_loss\": self.val_onset_loss_tracker.result(),\n            \"offset_loss\": self.val_offset_loss_tracker.result(),\n            \"frame_loss\": self.val_frame_loss_tracker.result()\n        }\n    \n    @property\n    def metrics(self):\n        return [\n            self.loss_tracker, self.onset_loss_tracker, self.offset_loss_tracker, self.frame_loss_tracker,\n            self.val_loss_tracker, self.val_onset_loss_tracker, self.val_offset_loss_tracker, self.val_frame_loss_tracker\n        ]\n\n# Training function\ndef train_model():\n    # Initialize data generators\n    train_gen = PianoDataGenerator('train', batch_size=BATCH_SIZE)\n    val_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE)\n    \n    # Estimate epochs to reach 2400 iterations\n    batches_per_epoch = len(train_gen)\n    total_epochs = 100  # Increased to 100 epochs as requested\n    logger.info(f\"Training for {total_epochs} epochs with {batches_per_epoch} batches/epoch\")\n    \n    # Initialize model\n    onsets_and_frames = OnsetsAndFrames(input_features=N_MELS, output_features=NOTES)\n    model = CustomModel(onsets_and_frames)\n    \n    # Learning rate schedule\n    initial_learning_rate = 0.00035  # As per your last code\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=10000 // batches_per_epoch,\n        decay_rate=0.99,  # Slower decay\n        staircase=True\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=None,\n        metrics=None\n    )\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        os.path.join(WORKING_DIR, \"best_model.keras\"),\n        monitor=\"val_loss\",\n        save_best_only=True,\n        verbose=1\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor=\"val_loss\",\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Custom callback to save metrics to CSV\n    class CSVMetricsCallback(tf.keras.callbacks.Callback):\n        def __init__(self, csv_file):\n            super().__init__()\n            self.csv_file = csv_file\n            self.metrics = []\n            if os.path.exists(csv_file):\n                os.remove(csv_file)\n            with open(csv_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow([\"epoch\", \"loss\", \"onset_loss\", \"offset_loss\", \"frame_loss\", \n                               \"val_loss\", \"val_onset_loss\", \"val_offset_loss\", \"val_frame_loss\"])\n        \n        def on_epoch_end(self, epoch, logs=None):\n            logs = logs or {}\n            row = [\n                epoch + 1,\n                logs.get('loss', 0),\n                logs.get('onset_loss', 0),\n                logs.get('offset_loss', 0),\n                logs.get('frame_loss', 0),\n                logs.get('val_loss', logs.get('val_val_loss', 0)),  # Handle both cases\n                logs.get('val_onset_loss', logs.get('val_val_onset_loss', 0)),\n                logs.get('val_offset_loss', logs.get('val_val_offset_loss', 0)),\n                logs.get('val_frame_loss', logs.get('val_val_frame_loss', 0))\n            ]\n            self.metrics.append(row)\n            with open(self.csv_file, 'a', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(row)\n    \n    # Train with validation\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=total_epochs,\n        callbacks=[\n            checkpoint,\n            early_stopping,\n            CSVMetricsCallback(CSV_FILE)\n        ],\n        verbose=1\n    )\n    \n    return model, history\n\n# Evaluation function\ndef evaluate_model(model, test_gen):\n    max_samples = min(10, len(test_gen))\n    Y_true_all, Y_pred_all = [], []\n    \n    for i in range(max_samples):\n        try:\n            batch_x, batch_y = test_gen[i]\n            batch_y = batch_y['frame']  # Use 'frame' key for evaluation\n        except ValueError as e:\n            print(f\"⏭️ Skipping batch {i}: {e}\")\n            continue\n        \n        preds = model.predict(batch_x, verbose=0)\n        pred_frame = preds['frame']\n        pred_frame = np.round(pred_frame).astype(int)\n        \n        Y_pred_all.append(pred_frame[0] if BATCH_SIZE == 1 else pred_frame)\n        Y_true_all.append(batch_y[0] if BATCH_SIZE == 1 else batch_y)\n    \n    Y_true_flat = np.concatenate(Y_true_all).reshape(-1)\n    Y_pred_flat = np.concatenate(Y_pred_all).reshape(-1)\n    \n    frame_precision = precision_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_recall = recall_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_f1 = f1_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    \n    print(\"\\n🎯 Frame-level Metrics:\")\n    print(f\"Precision: {frame_precision:.4f}\")\n    print(f\"Recall: {frame_recall:.4f}\")\n    print(f\"F1-score: {frame_f1:.4f}\")\n    \n    if frame_f1 >= 0.9277:\n        print(\"\\nFrame F1 Score exceeds CR-GCN's 92.77% - Excellent!\")\n    else:\n        print(f\"\\nFrame F1 Score is {frame_f1:.4f}, below CR-GCN's 92.77%. Consider further optimization.\")\n    \n    return frame_f1\n\n# Plot training vs validation loss\ndef plot_training_validation_curve(csv_file):\n    if not os.path.exists(csv_file):\n        print(f\"Error: {csv_file} not found. Ensure training completed.\")\n        return\n    \n    df = pd.read_csv(csv_file)\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['epoch'], df['loss'], label='Training Loss')\n    plt.plot(df['epoch'], df['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training vs Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(WORKING_DIR, 'training_validation_loss.png'))  # Save plot\n    plt.show()\n\nif __name__ == '__main__':\n    # Verify data directories\n    verify_data_dirs()\n    \n    # Train the model\n    model, history = train_model()\n    \n    # Evaluate the model\n    test_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE, shuffle=False)\n    evaluate_model(model, test_gen)\n    \n    # Save the final model\n    model.save(os.path.join(WORKING_DIR, \"final_model.keras\"))\n    logger.info(f\"Saved final model to {os.path.join(WORKING_DIR, 'final_model.keras')}\")\n    \n    # Plot training vs validation curve\n    plot_training_validation_curve(CSV_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:02:59.009599Z","iopub.execute_input":"2025-04-20T15:02:59.009926Z","iopub.status.idle":"2025-04-20T16:09:47.372063Z","shell.execute_reply.started":"2025-04-20T15:02:59.009902Z","shell.execute_reply":"2025-04-20T16:09:47.371097Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745161401.998353      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/custom_model_4_1/onsets_and_frames_4_1/sequential_29_1/sequential_28_1/dropout_36_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.3859 - loss: 0.2666 - offset_loss: 0.2050 - onset_loss: 0.2090\nEpoch 1: val_loss improved from inf to 0.17032, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 1s/step - frame_loss: 0.3848 - loss: 0.2660 - offset_loss: 0.2046 - onset_loss: 0.2086 - val_frame_loss: 0.1471 - val_loss: 0.1703 - val_offset_loss: 0.1842 - val_onset_loss: 0.1797\nEpoch 2/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1586 - loss: 0.1248 - offset_loss: 0.1072 - onset_loss: 0.1086\nEpoch 2: val_loss did not improve from 0.17032\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.1585 - loss: 0.1247 - offset_loss: 0.1072 - onset_loss: 0.1085 - val_frame_loss: 0.1602 - val_loss: 0.2143 - val_offset_loss: 0.2444 - val_onset_loss: 0.2384\nEpoch 3/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1444 - loss: 0.1044 - offset_loss: 0.0843 - onset_loss: 0.0846\nEpoch 3: val_loss did not improve from 0.17032\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.1443 - loss: 0.1044 - offset_loss: 0.0842 - onset_loss: 0.0845 - val_frame_loss: 0.1987 - val_loss: 0.2371 - val_offset_loss: 0.2567 - val_onset_loss: 0.2560\nEpoch 4/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1270 - loss: 0.0889 - offset_loss: 0.0696 - onset_loss: 0.0699\nEpoch 4: val_loss did not improve from 0.17032\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.1270 - loss: 0.0889 - offset_loss: 0.0696 - onset_loss: 0.0699 - val_frame_loss: 0.2083 - val_loss: 0.1813 - val_offset_loss: 0.1499 - val_onset_loss: 0.1858\nEpoch 5/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.1112 - loss: 0.0788 - offset_loss: 0.0625 - onset_loss: 0.0628\nEpoch 5: val_loss improved from 0.17032 to 0.15464, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.1112 - loss: 0.0788 - offset_loss: 0.0625 - onset_loss: 0.0628 - val_frame_loss: 0.2271 - val_loss: 0.1546 - val_offset_loss: 0.1036 - val_onset_loss: 0.1332\nEpoch 6/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0973 - loss: 0.0702 - offset_loss: 0.0568 - onset_loss: 0.0566\nEpoch 6: val_loss improved from 0.15464 to 0.13381, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.0973 - loss: 0.0702 - offset_loss: 0.0568 - onset_loss: 0.0566 - val_frame_loss: 0.1855 - val_loss: 0.1338 - val_offset_loss: 0.0813 - val_onset_loss: 0.1347\nEpoch 7/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0840 - loss: 0.0619 - offset_loss: 0.0510 - onset_loss: 0.0509\nEpoch 7: val_loss improved from 0.13381 to 0.07942, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.0840 - loss: 0.0619 - offset_loss: 0.0510 - onset_loss: 0.0509 - val_frame_loss: 0.1005 - val_loss: 0.0794 - val_offset_loss: 0.0659 - val_onset_loss: 0.0718\nEpoch 8/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0762 - loss: 0.0578 - offset_loss: 0.0486 - onset_loss: 0.0487\nEpoch 8: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0762 - loss: 0.0578 - offset_loss: 0.0486 - onset_loss: 0.0487 - val_frame_loss: 0.1801 - val_loss: 0.1517 - val_offset_loss: 0.1589 - val_onset_loss: 0.1162\nEpoch 9/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0679 - loss: 0.0523 - offset_loss: 0.0445 - onset_loss: 0.0445\nEpoch 9: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - frame_loss: 0.0679 - loss: 0.0523 - offset_loss: 0.0445 - onset_loss: 0.0446 - val_frame_loss: 0.0947 - val_loss: 0.0849 - val_offset_loss: 0.0912 - val_onset_loss: 0.0687\nEpoch 10/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0638 - loss: 0.0509 - offset_loss: 0.0444 - onset_loss: 0.0444\nEpoch 10: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0638 - loss: 0.0508 - offset_loss: 0.0444 - onset_loss: 0.0443 - val_frame_loss: 0.1569 - val_loss: 0.1508 - val_offset_loss: 0.2062 - val_onset_loss: 0.0894\nEpoch 11/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0559 - loss: 0.0454 - offset_loss: 0.0403 - onset_loss: 0.0400\nEpoch 11: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0559 - loss: 0.0454 - offset_loss: 0.0403 - onset_loss: 0.0400 - val_frame_loss: 0.2162 - val_loss: 0.1796 - val_offset_loss: 0.1864 - val_onset_loss: 0.1361\nEpoch 12/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0500 - loss: 0.0416 - offset_loss: 0.0374 - onset_loss: 0.0375\nEpoch 12: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0500 - loss: 0.0416 - offset_loss: 0.0374 - onset_loss: 0.0375 - val_frame_loss: 0.1603 - val_loss: 0.1415 - val_offset_loss: 0.1776 - val_onset_loss: 0.0868\nEpoch 13/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0480 - loss: 0.0406 - offset_loss: 0.0370 - onset_loss: 0.0368\nEpoch 13: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0480 - loss: 0.0406 - offset_loss: 0.0370 - onset_loss: 0.0368 - val_frame_loss: 0.1245 - val_loss: 0.1294 - val_offset_loss: 0.1873 - val_onset_loss: 0.0764\nEpoch 14/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0433 - loss: 0.0374 - offset_loss: 0.0345 - onset_loss: 0.0345\nEpoch 14: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0433 - loss: 0.0375 - offset_loss: 0.0345 - onset_loss: 0.0345 - val_frame_loss: 0.1779 - val_loss: 0.1602 - val_offset_loss: 0.1477 - val_onset_loss: 0.1549\nEpoch 15/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0431 - loss: 0.0378 - offset_loss: 0.0352 - onset_loss: 0.0350\nEpoch 15: val_loss did not improve from 0.07942\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0431 - loss: 0.0378 - offset_loss: 0.0352 - onset_loss: 0.0350 - val_frame_loss: 0.2071 - val_loss: 0.1774 - val_offset_loss: 0.1550 - val_onset_loss: 0.1703\nEpoch 16/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0385 - loss: 0.0340 - offset_loss: 0.0316 - onset_loss: 0.0319\nEpoch 16: val_loss improved from 0.07942 to 0.07624, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.0385 - loss: 0.0340 - offset_loss: 0.0316 - onset_loss: 0.0320 - val_frame_loss: 0.0857 - val_loss: 0.0762 - val_offset_loss: 0.0783 - val_onset_loss: 0.0647\nEpoch 17/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0381 - loss: 0.0338 - offset_loss: 0.0317 - onset_loss: 0.0316\nEpoch 17: val_loss improved from 0.07624 to 0.07197, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.0381 - loss: 0.0338 - offset_loss: 0.0317 - onset_loss: 0.0316 - val_frame_loss: 0.0872 - val_loss: 0.0720 - val_offset_loss: 0.0652 - val_onset_loss: 0.0635\nEpoch 18/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0346 - loss: 0.0315 - offset_loss: 0.0300 - onset_loss: 0.0299\nEpoch 18: val_loss did not improve from 0.07197\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0346 - loss: 0.0315 - offset_loss: 0.0300 - onset_loss: 0.0299 - val_frame_loss: 0.1075 - val_loss: 0.0889 - val_offset_loss: 0.0663 - val_onset_loss: 0.0930\nEpoch 19/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0323 - loss: 0.0302 - offset_loss: 0.0291 - onset_loss: 0.0292\nEpoch 19: val_loss improved from 0.07197 to 0.06323, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - frame_loss: 0.0323 - loss: 0.0302 - offset_loss: 0.0291 - onset_loss: 0.0292 - val_frame_loss: 0.0711 - val_loss: 0.0632 - val_offset_loss: 0.0607 - val_onset_loss: 0.0578\nEpoch 20/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0330 - loss: 0.0308 - offset_loss: 0.0297 - onset_loss: 0.0299\nEpoch 20: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0330 - loss: 0.0308 - offset_loss: 0.0297 - onset_loss: 0.0298 - val_frame_loss: 0.0987 - val_loss: 0.0849 - val_offset_loss: 0.0932 - val_onset_loss: 0.0628\nEpoch 21/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0302 - loss: 0.0284 - offset_loss: 0.0276 - onset_loss: 0.0275\nEpoch 21: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0302 - loss: 0.0284 - offset_loss: 0.0276 - onset_loss: 0.0275 - val_frame_loss: 0.0990 - val_loss: 0.0750 - val_offset_loss: 0.0628 - val_onset_loss: 0.0633\nEpoch 22/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0275 - loss: 0.0266 - offset_loss: 0.0261 - onset_loss: 0.0261\nEpoch 22: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0276 - loss: 0.0266 - offset_loss: 0.0261 - onset_loss: 0.0261 - val_frame_loss: 0.1496 - val_loss: 0.1214 - val_offset_loss: 0.1285 - val_onset_loss: 0.0860\nEpoch 23/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0270 - loss: 0.0261 - offset_loss: 0.0257 - onset_loss: 0.0256\nEpoch 23: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0270 - loss: 0.0261 - offset_loss: 0.0257 - onset_loss: 0.0256 - val_frame_loss: 0.0875 - val_loss: 0.0765 - val_offset_loss: 0.0834 - val_onset_loss: 0.0588\nEpoch 24/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0261 - loss: 0.0254 - offset_loss: 0.0251 - onset_loss: 0.0250\nEpoch 24: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0261 - loss: 0.0254 - offset_loss: 0.0251 - onset_loss: 0.0250 - val_frame_loss: 0.0921 - val_loss: 0.1012 - val_offset_loss: 0.1512 - val_onset_loss: 0.0602\nEpoch 25/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0242 - loss: 0.0241 - offset_loss: 0.0242 - onset_loss: 0.0240\nEpoch 25: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0242 - loss: 0.0241 - offset_loss: 0.0242 - onset_loss: 0.0241 - val_frame_loss: 0.0929 - val_loss: 0.0820 - val_offset_loss: 0.0862 - val_onset_loss: 0.0670\nEpoch 26/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0242 - loss: 0.0243 - offset_loss: 0.0244 - onset_loss: 0.0244\nEpoch 26: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0242 - loss: 0.0243 - offset_loss: 0.0244 - onset_loss: 0.0244 - val_frame_loss: 0.1040 - val_loss: 0.0884 - val_offset_loss: 0.0971 - val_onset_loss: 0.0641\nEpoch 27/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0233 - loss: 0.0235 - offset_loss: 0.0237 - onset_loss: 0.0236\nEpoch 27: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0233 - loss: 0.0235 - offset_loss: 0.0237 - onset_loss: 0.0236 - val_frame_loss: 0.0824 - val_loss: 0.0771 - val_offset_loss: 0.0842 - val_onset_loss: 0.0648\nEpoch 28/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0228 - loss: 0.0232 - offset_loss: 0.0233 - onset_loss: 0.0235\nEpoch 28: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0228 - loss: 0.0232 - offset_loss: 0.0233 - onset_loss: 0.0235 - val_frame_loss: 0.0739 - val_loss: 0.0656 - val_offset_loss: 0.0598 - val_onset_loss: 0.0630\nEpoch 29/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - frame_loss: 0.0210 - loss: 0.0218 - offset_loss: 0.0223 - onset_loss: 0.0221\nEpoch 29: val_loss did not improve from 0.06323\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - frame_loss: 0.0210 - loss: 0.0218 - offset_loss: 0.0223 - onset_loss: 0.0221 - val_frame_loss: 0.0844 - val_loss: 0.0728 - val_offset_loss: 0.0637 - val_onset_loss: 0.0703\nEpoch 29: early stopping\nRestoring model weights from the end of the best epoch: 19.\n\n🎯 Frame-level Metrics:\nPrecision: 0.7835\nRecall: 0.6173\nF1-score: 0.6906\n\nFrame F1 Score is 0.6906, below CR-GCN's 92.77%. Consider further optimization.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3475323070.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;31m# Plot training vs validation curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0mplot_training_validation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/3475323070.py\u001b[0m in \u001b[0;36mplot_training_validation_curve\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"],"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport logging\nimport csv\nimport matplotlib.pyplot as plt\n\n# Suppress TensorFlow CUDA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Constants\nSR = 16000\nN_MELS = 229\nHOP_LENGTH = 512\nTARGET_FRAMES = 480\nNOTES = 88\nINPUT_DIR = \"/kaggle/input\"  # Base input directory\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"preprocessed\", \"preprocessed\")  # Point to the inner preprocessed directory\nWORKING_DIR = \"/kaggle/working\"  # Where to save models\nBATCH_SIZE = 8\nITERATIONS = 2400  # Adjusted to target ~50 epochs with 120 batches/epoch\nCHECKPOINT_INTERVAL = 240  # Save checkpoint every 240 steps\nCSV_FILE = os.path.join(WORKING_DIR, \"training_metrics.csv\")\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify directory structure\ndef verify_data_dirs():\n    expected_dirs = [\n        os.path.join(OUTPUT_DIR, 'train', 'mel'),\n        os.path.join(OUTPUT_DIR, 'train', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'validation', 'mel'),\n        os.path.join(OUTPUT_DIR, 'validation', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'test', 'mel'),\n        os.path.join(OUTPUT_DIR, 'test', 'piano_roll')\n    ]\n    for dir_path in expected_dirs:\n        if not os.path.exists(dir_path):\n            logger.error(f\"Directory {dir_path} not found.\")\n            logger.error(f\"Contents of {os.path.dirname(dir_path)}: {os.listdir(os.path.dirname(dir_path))}\")\n            raise FileNotFoundError(f\"Directory {dir_path} not found. Check dataset structure in {OUTPUT_DIR}.\")\n        if not os.listdir(dir_path):\n            raise FileNotFoundError(f\"Directory {dir_path} is empty. Check dataset contents.\")\n\n# Normalize adjacency matrix\ndef normalize_adj(A, symmetric=True):\n    A = A + tf.eye(tf.shape(A)[0], dtype=tf.float32)\n    d = tf.reduce_sum(A, axis=1)\n    if symmetric:\n        D_inv_sqrt = tf.linalg.diag(tf.pow(d, -0.5))\n        return D_inv_sqrt @ A @ D_inv_sqrt\n    else:\n        D_inv = tf.linalg.diag(tf.pow(d, -1))\n        return D_inv @ A\n\n# Compute adjacency matrix\ndef compute_adjacency_matrix(train_roll_dir):\n    train_rolls = []\n    for f in os.listdir(train_roll_dir):\n        if f.endswith('.npy'):\n            roll = np.load(os.path.join(train_roll_dir, f))\n            train_rolls.append(roll)\n    \n    all_frames = np.concatenate(train_rolls, axis=1)\n    all_frames = (all_frames > 0).astype(np.float32)\n    \n    M = tf.matmul(all_frames, all_frames, transpose_b=True)\n    M = M + tf.transpose(M)\n    M_max = tf.reduce_max(M)\n    P = M / M_max if M_max > 0 else M\n    A = tf.cast(P >= 0.6, tf.float32)\n    A = tf.cast((A + tf.transpose(A)) > 0, tf.float32)\n    A = normalize_adj(A, symmetric=True)\n    \n    return A\n\n# GCN Layer\nclass GCNSimple(tf.keras.layers.Layer):\n    def __init__(self, dim_in, dim_out, **kwargs):\n        super(GCNSimple, self).__init__(**kwargs)\n        train_roll_dir = os.path.join(OUTPUT_DIR, 'train', 'piano_roll')\n        self.A = tf.constant(compute_adjacency_matrix(train_roll_dir), dtype=tf.float32)\n        self.fc1 = layers.Dense(dim_in, use_bias=False)\n        self.fc2 = layers.Dense(dim_out // 2, use_bias=False)\n        self.fc3 = layers.Dense(dim_out, use_bias=False)\n    \n    def call(self, inputs):\n        X = tf.nn.relu(self.fc1(self.A))\n        X = tf.nn.relu(self.fc2(tf.matmul(self.A, X)))\n        X = self.fc3(tf.matmul(self.A, X))\n        return tf.matmul(inputs, X)\n    \n    def get_config(self):\n        config = super(GCNSimple, self).get_config()\n        config.update({'dim_in': self.fc1.units, 'dim_out': self.fc3.units})\n        return config\n\n# ConvStack\ndef build_conv_stack(input_features, output_features):\n    model = models.Sequential([\n        layers.Input(shape=(None, input_features, 1)),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.4),  # Increased to 0.4\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.4),  # Increased to 0.4\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),  # Extra layer\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Reshape((-1, (output_features // 8) * (input_features // 4))),\n        layers.Dense(output_features, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        layers.Dropout(0.4)\n    ])\n    return model\n\n# OnsetsAndFrames model\nclass OnsetsAndFrames(Model):\n    def __init__(self, input_features, output_features, model_complexity=96, **kwargs):  # Increased to 96\n        super(OnsetsAndFrames, self).__init__(**kwargs)\n        self.input_features = input_features\n        self.output_features = output_features\n        self.model_complexity = model_complexity\n        model_size = model_complexity * 16\n        \n        self.onset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.offset_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True)),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.frame_stack = models.Sequential([\n            build_conv_stack(input_features, model_size),\n            layers.Dense(output_features),\n            layers.Activation('sigmoid')\n        ])\n        \n        self.combined_stack = models.Sequential([\n            layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True), \n                               input_shape=(None, output_features * 3)),\n            layers.Dense(output_features),\n            GCNSimple(output_features, output_features),\n            layers.Activation('sigmoid')\n        ])\n    \n    def call(self, mel, training=False):\n        mel = tf.expand_dims(mel, -1)\n        onset_pred = self.onset_stack(mel, training=training)\n        offset_pred = self.offset_stack(mel, training=training)\n        activation_pred = self.frame_stack(mel, training=training)\n        combined_pred = tf.concat([onset_pred, offset_pred, activation_pred], axis=-1)\n        frame_pred = self.combined_stack(combined_pred, training=training)\n        return {'onset': onset_pred, 'offset': offset_pred, 'frame': frame_pred}\n    \n    def get_config(self):\n        config = super(OnsetsAndFrames, self).get_config()\n        config.update({\n            'input_features': self.input_features,\n            'output_features': self.output_features,\n            'model_complexity': self.model_complexity\n        })\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        input_features = config.get('input_features', N_MELS)\n        output_features = config.get('output_features', NOTES)\n        model_complexity = config.get('model_complexity', 96)\n        return cls(input_features=input_features, \n                   output_features=output_features, \n                   model_complexity=model_complexity)\n\n# Data generator\nclass PianoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, split, batch_size=8, shuffle=False):\n        self.split = split\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.audio_dir = os.path.join(OUTPUT_DIR, split, 'mel')\n        self.roll_dir = os.path.join(OUTPUT_DIR, split, 'piano_roll')\n        self.audio_files = [f for f in os.listdir(self.audio_dir) if f.endswith('.npy')]\n        self.roll_files = [f for f in os.listdir(self.roll_dir) if f.endswith('.npy')]\n        self.audio_files.sort()\n        self.roll_files.sort()\n        assert len(self.audio_files) == len(self.roll_files), f\"Mismatch in {split}\"\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.audio_files) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_audio_files = self.audio_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_roll_files = self.roll_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.array([np.load(os.path.join(self.audio_dir, f)) for f in batch_audio_files])\n        y = np.array([np.load(os.path.join(self.roll_dir, f)) for f in batch_roll_files])\n        \n        # Ensure 3D shape\n        if X.ndim == 2:\n            X = np.expand_dims(X, axis=0)\n        if y.ndim == 2:\n            y = np.expand_dims(y, axis=0)\n        \n        # Normalize X to [0, 1] and add light noise\n        X = (X - np.min(X)) / (np.max(X) - np.min(X) + 1e-8)\n        X = X + np.random.normal(0, 0.01, X.shape)  # Add small Gaussian noise\n        \n        # Transpose to (batch, frames, features)\n        if X.shape[1] == N_MELS and X.shape[2] == TARGET_FRAMES:\n            X = np.transpose(X, (0, 2, 1))\n        elif X.shape[1] != TARGET_FRAMES or X.shape[2] != N_MELS:\n            raise ValueError(f\"Unexpected X shape: {X.shape}\")\n        \n        if y.shape[1] == NOTES and y.shape[2] == TARGET_FRAMES:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.shape[1] != TARGET_FRAMES or y.shape[2] != NOTES:\n            raise ValueError(f\"Unexpected y shape: {y.shape}\")\n        \n        logger.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n        return X, {'onset': y, 'offset': y, 'frame': y}\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            indices = np.arange(len(self.audio_files))\n            np.random.shuffle(indices)\n            self.audio_files = [self.audio_files[i] for i in indices]\n            self.roll_files = [self.roll_files[i] for i in indices]\n\n# Custom training step with loss only\nclass CustomModel(Model):\n    def __init__(self, onsets_and_frames_model):\n        super(CustomModel, self).__init__()\n        self.onsets_and_frames = onsets_and_frames_model\n        self.clip_norm = 3.0\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.onset_loss_tracker = tf.keras.metrics.Mean(name=\"onset_loss\")\n        self.offset_loss_tracker = tf.keras.metrics.Mean(name=\"offset_loss\")\n        self.frame_loss_tracker = tf.keras.metrics.Mean(name=\"frame_loss\")\n        self.val_loss_tracker = tf.keras.metrics.Mean(name=\"val_loss\")\n        self.val_onset_loss_tracker = tf.keras.metrics.Mean(name=\"val_onset_loss\")\n        self.val_offset_loss_tracker = tf.keras.metrics.Mean(name=\"val_offset_loss\")\n        self.val_frame_loss_tracker = tf.keras.metrics.Mean(name=\"val_frame_loss\")\n    \n    def call(self, inputs, training=False):\n        return self.onsets_and_frames(inputs, training=training)\n    \n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        losses = {}\n        for key in ['onset', 'offset', 'frame']:\n            losses[key] = tf.keras.losses.binary_crossentropy(y[key], y_pred[key])\n        total_loss = sum(losses.values()) / len(losses)\n        return total_loss, losses['onset'], losses['offset'], losses['frame']\n    \n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(total_loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, self.clip_norm) if g is not None else g for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        self.loss_tracker.update_state(total_loss)\n        self.onset_loss_tracker.update_state(onset_loss)\n        self.offset_loss_tracker.update_state(offset_loss)\n        self.frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.loss_tracker.result(),\n            \"onset_loss\": self.onset_loss_tracker.result(),\n            \"offset_loss\": self.offset_loss_tracker.result(),\n            \"frame_loss\": self.frame_loss_tracker.result()\n        }\n    \n    def test_step(self, data):\n        x, y = data\n        y_pred = self(x, training=False)\n        total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred)\n        \n        self.val_loss_tracker.update_state(total_loss)\n        self.val_onset_loss_tracker.update_state(onset_loss)\n        self.val_offset_loss_tracker.update_state(offset_loss)\n        self.val_frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.val_loss_tracker.result(),\n            \"onset_loss\": self.val_onset_loss_tracker.result(),\n            \"offset_loss\": self.val_offset_loss_tracker.result(),\n            \"frame_loss\": self.val_frame_loss_tracker.result()\n        }\n    \n    @property\n    def metrics(self):\n        return [\n            self.loss_tracker, self.onset_loss_tracker, self.offset_loss_tracker, self.frame_loss_tracker,\n            self.val_loss_tracker, self.val_onset_loss_tracker, self.val_offset_loss_tracker, self.val_frame_loss_tracker\n        ]\n\n# Training function\ndef train_model():\n    train_gen = PianoDataGenerator('train', batch_size=BATCH_SIZE)\n    val_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE)\n    \n    batches_per_epoch = len(train_gen)\n    total_epochs = 100\n    logger.info(f\"Training for {total_epochs} epochs with {batches_per_epoch} batches/epoch\")\n    \n    onsets_and_frames = OnsetsAndFrames(input_features=N_MELS, output_features=NOTES)\n    model = CustomModel(onsets_and_frames)\n    \n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=0.0001,\n        decay_steps=total_epochs * batches_per_epoch\n    )\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=None,\n        metrics=None\n    )\n    \n    checkpoint = ModelCheckpoint(\n        os.path.join(WORKING_DIR, \"best_model.keras\"),\n        monitor=\"val_loss\",\n        save_best_only=True,\n        verbose=1\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor=\"val_loss\",\n        patience=20,  # Increased to 20\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    class CSVMetricsCallback(tf.keras.callbacks.Callback):\n        def __init__(self, csv_file):\n            super().__init__()\n            self.csv_file = csv_file\n            self.metrics = []\n            if os.path.exists(csv_file):\n                os.remove(csv_file)\n            with open(csv_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow([\"epoch\", \"loss\", \"onset_loss\", \"offset_loss\", \"frame_loss\", \n                               \"val_loss\", \"val_onset_loss\", \"val_offset_loss\", \"val_frame_loss\"])\n        \n        def on_epoch_end(self, epoch, logs=None):\n            logs = logs or {}\n            row = [\n                epoch + 1,\n                logs.get('loss', 0),\n                logs.get('onset_loss', 0),\n                logs.get('offset_loss', 0),\n                logs.get('frame_loss', 0),\n                logs.get('val_loss', logs.get('val_val_loss', 0)),\n                logs.get('val_onset_loss', logs.get('val_val_onset_loss', 0)),\n                logs.get('val_offset_loss', logs.get('val_val_offset_loss', 0)),\n                logs.get('val_frame_loss', logs.get('val_val_frame_loss', 0))\n            ]\n            self.metrics.append(row)\n            with open(self.csv_file, 'a', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(row)\n    \n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=total_epochs,\n        callbacks=[checkpoint, early_stopping, CSVMetricsCallback(CSV_FILE)],\n        verbose=1\n    )\n    \n    return model, history\n\n# Evaluation function\ndef evaluate_model(model, test_gen):\n    max_samples = min(10, len(test_gen))\n    Y_true_all, Y_pred_all = [], []\n    \n    for i in range(max_samples):\n        try:\n            batch_x, batch_y = test_gen[i]\n            batch_y = batch_y['frame']\n        except ValueError as e:\n            print(f\"⏭️ Skipping batch {i}: {e}\")\n            continue\n        \n        preds = model.predict(batch_x, verbose=0)\n        pred_frame = preds['frame']\n        pred_frame = np.where(pred_frame > 0.3, 1, 0)  # Adjusted threshold to 0.3 for recall\n        \n        Y_pred_all.append(pred_frame[0] if BATCH_SIZE == 1 else pred_frame)\n        Y_true_all.append(batch_y[0] if BATCH_SIZE == 1 else batch_y)\n    \n    Y_true_flat = np.concatenate(Y_true_all).reshape(-1)\n    Y_pred_flat = np.concatenate(Y_pred_all).reshape(-1)\n    \n    frame_precision = precision_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_recall = recall_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    frame_f1 = f1_score(Y_true_flat, Y_pred_flat, zero_division=0)\n    \n    print(\"\\n🎯 Frame-level Metrics:\")\n    print(f\"Precision: {frame_precision:.4f}\")\n    print(f\"Recall: {frame_recall:.4f}\")\n    print(f\"F1-score: {frame_f1:.4f}\")\n    \n    if frame_f1 >= 0.9277:\n        print(\"\\nFrame F1 Score exceeds CR-GCN's 92.77% - Excellent!\")\n    else:\n        print(f\"\\nFrame F1 Score is {frame_f1:.4f}, below CR-GCN's 92.77%. Consider further optimization.\")\n    \n    return frame_f1\n\n# Plot training vs validation loss\ndef plot_training_validation_curve(csv_file):\n    if not os.path.exists(csv_file):\n        print(f\"Error: {csv_file} not found. Ensure training completed.\")\n        return\n    \n    df = pd.read_csv(csv_file)\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['epoch'], df['loss'], label='Training Loss')\n    plt.plot(df['epoch'], df['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training vs Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(WORKING_DIR, 'training_validation_loss.png'))\n    plt.show()\n\nif __name__ == '__main__':\n    verify_data_dirs()\n    model, history = train_model()\n    test_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE, shuffle=False)\n    evaluate_model(model, test_gen)\n    model.save(os.path.join(WORKING_DIR, \"final_model.keras\"))\n    logger.info(f\"Saved final model to {os.path.join(WORKING_DIR, 'final_model.keras')}\")\n    plot_training_validation_curve(CSV_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:26:08.243348Z","iopub.execute_input":"2025-04-20T16:26:08.243659Z","iopub.status.idle":"2025-04-20T21:17:38.220095Z","shell.execute_reply.started":"2025-04-20T16:26:08.243635Z","shell.execute_reply":"2025-04-20T21:17:38.219363Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745166396.614595      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/custom_model_5_1/onsets_and_frames_5_1/sequential_36_1/sequential_35_1/dropout_45_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.4705 - loss: 0.2883 - offset_loss: 0.1973 - onset_loss: 0.1969\nEpoch 1: val_loss improved from inf to 0.18858, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 3s/step - frame_loss: 0.4695 - loss: 0.2876 - offset_loss: 0.1969 - onset_loss: 0.1965 - val_frame_loss: 0.1802 - val_loss: 0.1886 - val_offset_loss: 0.1905 - val_onset_loss: 0.1950\nEpoch 2/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1735 - loss: 0.1204 - offset_loss: 0.0942 - onset_loss: 0.0936\nEpoch 2: val_loss did not improve from 0.18858\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 3s/step - frame_loss: 0.1735 - loss: 0.1204 - offset_loss: 0.0941 - onset_loss: 0.0936 - val_frame_loss: 0.1480 - val_loss: 0.2108 - val_offset_loss: 0.2395 - val_onset_loss: 0.2451\nEpoch 3/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1542 - loss: 0.1005 - offset_loss: 0.0735 - onset_loss: 0.0736\nEpoch 3: val_loss did not improve from 0.18858\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.1542 - loss: 0.1004 - offset_loss: 0.0735 - onset_loss: 0.0736 - val_frame_loss: 0.1499 - val_loss: 0.2187 - val_offset_loss: 0.2493 - val_onset_loss: 0.2570\nEpoch 4/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1482 - loss: 0.0892 - offset_loss: 0.0593 - onset_loss: 0.0601\nEpoch 4: val_loss improved from 0.18858 to 0.17101, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 3s/step - frame_loss: 0.1482 - loss: 0.0892 - offset_loss: 0.0593 - onset_loss: 0.0601 - val_frame_loss: 0.1516 - val_loss: 0.1710 - val_offset_loss: 0.1749 - val_onset_loss: 0.1865\nEpoch 5/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1413 - loss: 0.0834 - offset_loss: 0.0547 - onset_loss: 0.0543\nEpoch 5: val_loss improved from 0.17101 to 0.14380, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.1413 - loss: 0.0834 - offset_loss: 0.0547 - onset_loss: 0.0543 - val_frame_loss: 0.1520 - val_loss: 0.1438 - val_offset_loss: 0.1252 - val_onset_loss: 0.1543\nEpoch 6/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1374 - loss: 0.0798 - offset_loss: 0.0505 - onset_loss: 0.0515\nEpoch 6: val_loss improved from 0.14380 to 0.10406, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 3s/step - frame_loss: 0.1373 - loss: 0.0798 - offset_loss: 0.0505 - onset_loss: 0.0515 - val_frame_loss: 0.1432 - val_loss: 0.1041 - val_offset_loss: 0.0861 - val_onset_loss: 0.0828\nEpoch 7/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1265 - loss: 0.0714 - offset_loss: 0.0436 - onset_loss: 0.0440\nEpoch 7: val_loss improved from 0.10406 to 0.08569, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.1265 - loss: 0.0714 - offset_loss: 0.0436 - onset_loss: 0.0441 - val_frame_loss: 0.1243 - val_loss: 0.0857 - val_offset_loss: 0.0692 - val_onset_loss: 0.0636\nEpoch 8/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1182 - loss: 0.0654 - offset_loss: 0.0388 - onset_loss: 0.0392\nEpoch 8: val_loss did not improve from 0.08569\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.1182 - loss: 0.0654 - offset_loss: 0.0388 - onset_loss: 0.0392 - val_frame_loss: 0.1175 - val_loss: 0.0860 - val_offset_loss: 0.0812 - val_onset_loss: 0.0592\nEpoch 9/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1115 - loss: 0.0630 - offset_loss: 0.0383 - onset_loss: 0.0390\nEpoch 9: val_loss did not improve from 0.08569\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.1115 - loss: 0.0630 - offset_loss: 0.0383 - onset_loss: 0.0390 - val_frame_loss: 0.1271 - val_loss: 0.0918 - val_offset_loss: 0.0758 - val_onset_loss: 0.0725\nEpoch 10/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.1034 - loss: 0.0583 - offset_loss: 0.0354 - onset_loss: 0.0360\nEpoch 10: val_loss improved from 0.08569 to 0.07314, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.1034 - loss: 0.0583 - offset_loss: 0.0354 - onset_loss: 0.0360 - val_frame_loss: 0.1008 - val_loss: 0.0731 - val_offset_loss: 0.0569 - val_onset_loss: 0.0617\nEpoch 11/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0946 - loss: 0.0536 - offset_loss: 0.0329 - onset_loss: 0.0334\nEpoch 11: val_loss improved from 0.07314 to 0.07120, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 3s/step - frame_loss: 0.0946 - loss: 0.0536 - offset_loss: 0.0329 - onset_loss: 0.0334 - val_frame_loss: 0.0957 - val_loss: 0.0712 - val_offset_loss: 0.0555 - val_onset_loss: 0.0623\nEpoch 12/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0890 - loss: 0.0515 - offset_loss: 0.0324 - onset_loss: 0.0332\nEpoch 12: val_loss did not improve from 0.07120\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0890 - loss: 0.0515 - offset_loss: 0.0324 - onset_loss: 0.0332 - val_frame_loss: 0.0894 - val_loss: 0.0796 - val_offset_loss: 0.0698 - val_onset_loss: 0.0797\nEpoch 13/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0798 - loss: 0.0471 - offset_loss: 0.0306 - onset_loss: 0.0309\nEpoch 13: val_loss did not improve from 0.07120\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0797 - loss: 0.0471 - offset_loss: 0.0306 - onset_loss: 0.0309 - val_frame_loss: 0.0896 - val_loss: 0.0817 - val_offset_loss: 0.0989 - val_onset_loss: 0.0565\nEpoch 14/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0727 - loss: 0.0437 - offset_loss: 0.0291 - onset_loss: 0.0294\nEpoch 14: val_loss improved from 0.07120 to 0.06474, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 3s/step - frame_loss: 0.0727 - loss: 0.0437 - offset_loss: 0.0291 - onset_loss: 0.0294 - val_frame_loss: 0.0806 - val_loss: 0.0647 - val_offset_loss: 0.0566 - val_onset_loss: 0.0571\nEpoch 15/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0673 - loss: 0.0409 - offset_loss: 0.0275 - onset_loss: 0.0280\nEpoch 15: val_loss did not improve from 0.06474\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0673 - loss: 0.0409 - offset_loss: 0.0275 - onset_loss: 0.0280 - val_frame_loss: 0.0922 - val_loss: 0.0827 - val_offset_loss: 0.0787 - val_onset_loss: 0.0773\nEpoch 16/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0586 - loss: 0.0365 - offset_loss: 0.0252 - onset_loss: 0.0255\nEpoch 16: val_loss did not improve from 0.06474\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0586 - loss: 0.0365 - offset_loss: 0.0252 - onset_loss: 0.0255 - val_frame_loss: 0.1027 - val_loss: 0.0992 - val_offset_loss: 0.0841 - val_onset_loss: 0.1107\nEpoch 17/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0549 - loss: 0.0348 - offset_loss: 0.0247 - onset_loss: 0.0249\nEpoch 17: val_loss improved from 0.06474 to 0.06283, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.0549 - loss: 0.0348 - offset_loss: 0.0247 - onset_loss: 0.0249 - val_frame_loss: 0.0739 - val_loss: 0.0628 - val_offset_loss: 0.0570 - val_onset_loss: 0.0576\nEpoch 18/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0492 - loss: 0.0319 - offset_loss: 0.0231 - onset_loss: 0.0234\nEpoch 18: val_loss improved from 0.06283 to 0.06079, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.0492 - loss: 0.0319 - offset_loss: 0.0231 - onset_loss: 0.0234 - val_frame_loss: 0.0716 - val_loss: 0.0608 - val_offset_loss: 0.0559 - val_onset_loss: 0.0549\nEpoch 19/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0464 - loss: 0.0305 - offset_loss: 0.0225 - onset_loss: 0.0227\nEpoch 19: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0464 - loss: 0.0305 - offset_loss: 0.0225 - onset_loss: 0.0227 - val_frame_loss: 0.0767 - val_loss: 0.0650 - val_offset_loss: 0.0620 - val_onset_loss: 0.0564\nEpoch 20/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0417 - loss: 0.0281 - offset_loss: 0.0211 - onset_loss: 0.0214\nEpoch 20: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0417 - loss: 0.0281 - offset_loss: 0.0211 - onset_loss: 0.0214 - val_frame_loss: 0.0894 - val_loss: 0.0774 - val_offset_loss: 0.0670 - val_onset_loss: 0.0758\nEpoch 21/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0399 - loss: 0.0273 - offset_loss: 0.0208 - onset_loss: 0.0210\nEpoch 21: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0399 - loss: 0.0273 - offset_loss: 0.0208 - onset_loss: 0.0210 - val_frame_loss: 0.0878 - val_loss: 0.0754 - val_offset_loss: 0.0622 - val_onset_loss: 0.0763\nEpoch 22/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0376 - loss: 0.0263 - offset_loss: 0.0205 - onset_loss: 0.0207\nEpoch 22: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0375 - loss: 0.0263 - offset_loss: 0.0205 - onset_loss: 0.0207 - val_frame_loss: 0.0719 - val_loss: 0.0613 - val_offset_loss: 0.0590 - val_onset_loss: 0.0531\nEpoch 23/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0340 - loss: 0.0241 - offset_loss: 0.0191 - onset_loss: 0.0194\nEpoch 23: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0340 - loss: 0.0241 - offset_loss: 0.0191 - onset_loss: 0.0194 - val_frame_loss: 0.0794 - val_loss: 0.0701 - val_offset_loss: 0.0738 - val_onset_loss: 0.0570\nEpoch 24/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0305 - loss: 0.0221 - offset_loss: 0.0177 - onset_loss: 0.0180\nEpoch 24: val_loss did not improve from 0.06079\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0305 - loss: 0.0221 - offset_loss: 0.0177 - onset_loss: 0.0180 - val_frame_loss: 0.0822 - val_loss: 0.0713 - val_offset_loss: 0.0666 - val_onset_loss: 0.0651\nEpoch 25/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0275 - loss: 0.0202 - offset_loss: 0.0164 - onset_loss: 0.0166\nEpoch 25: val_loss improved from 0.06079 to 0.06071, saving model to /kaggle/working/best_model.keras\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - frame_loss: 0.0275 - loss: 0.0202 - offset_loss: 0.0164 - onset_loss: 0.0166 - val_frame_loss: 0.0731 - val_loss: 0.0607 - val_offset_loss: 0.0536 - val_onset_loss: 0.0554\nEpoch 26/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0277 - loss: 0.0207 - offset_loss: 0.0171 - onset_loss: 0.0172\nEpoch 26: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0277 - loss: 0.0207 - offset_loss: 0.0171 - onset_loss: 0.0172 - val_frame_loss: 0.0785 - val_loss: 0.0680 - val_offset_loss: 0.0627 - val_onset_loss: 0.0627\nEpoch 27/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0263 - loss: 0.0198 - offset_loss: 0.0164 - onset_loss: 0.0165\nEpoch 27: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 3s/step - frame_loss: 0.0263 - loss: 0.0198 - offset_loss: 0.0164 - onset_loss: 0.0165 - val_frame_loss: 0.0804 - val_loss: 0.0690 - val_offset_loss: 0.0666 - val_onset_loss: 0.0600\nEpoch 28/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0235 - loss: 0.0181 - offset_loss: 0.0154 - onset_loss: 0.0155\nEpoch 28: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0235 - loss: 0.0181 - offset_loss: 0.0154 - onset_loss: 0.0155 - val_frame_loss: 0.0769 - val_loss: 0.0628 - val_offset_loss: 0.0538 - val_onset_loss: 0.0578\nEpoch 29/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0223 - loss: 0.0172 - offset_loss: 0.0147 - onset_loss: 0.0147\nEpoch 29: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0223 - loss: 0.0172 - offset_loss: 0.0147 - onset_loss: 0.0148 - val_frame_loss: 0.0791 - val_loss: 0.0660 - val_offset_loss: 0.0547 - val_onset_loss: 0.0644\nEpoch 30/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0214 - loss: 0.0169 - offset_loss: 0.0145 - onset_loss: 0.0147\nEpoch 30: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0214 - loss: 0.0169 - offset_loss: 0.0145 - onset_loss: 0.0147 - val_frame_loss: 0.0758 - val_loss: 0.0631 - val_offset_loss: 0.0527 - val_onset_loss: 0.0608\nEpoch 31/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0209 - loss: 0.0166 - offset_loss: 0.0143 - onset_loss: 0.0146\nEpoch 31: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0209 - loss: 0.0166 - offset_loss: 0.0143 - onset_loss: 0.0146 - val_frame_loss: 0.0806 - val_loss: 0.0645 - val_offset_loss: 0.0535 - val_onset_loss: 0.0594\nEpoch 32/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0204 - loss: 0.0164 - offset_loss: 0.0142 - onset_loss: 0.0144\nEpoch 32: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0204 - loss: 0.0164 - offset_loss: 0.0142 - onset_loss: 0.0144 - val_frame_loss: 0.0880 - val_loss: 0.0670 - val_offset_loss: 0.0557 - val_onset_loss: 0.0572\nEpoch 33/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0181 - loss: 0.0147 - offset_loss: 0.0130 - onset_loss: 0.0131\nEpoch 33: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0181 - loss: 0.0147 - offset_loss: 0.0130 - onset_loss: 0.0131 - val_frame_loss: 0.0906 - val_loss: 0.0712 - val_offset_loss: 0.0658 - val_onset_loss: 0.0573\nEpoch 34/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0174 - loss: 0.0146 - offset_loss: 0.0131 - onset_loss: 0.0132\nEpoch 34: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0174 - loss: 0.0146 - offset_loss: 0.0131 - onset_loss: 0.0132 - val_frame_loss: 0.0864 - val_loss: 0.0667 - val_offset_loss: 0.0578 - val_onset_loss: 0.0560\nEpoch 35/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0163 - loss: 0.0137 - offset_loss: 0.0123 - onset_loss: 0.0125\nEpoch 35: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0163 - loss: 0.0137 - offset_loss: 0.0123 - onset_loss: 0.0125 - val_frame_loss: 0.0783 - val_loss: 0.0623 - val_offset_loss: 0.0547 - val_onset_loss: 0.0538\nEpoch 36/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0164 - loss: 0.0140 - offset_loss: 0.0128 - onset_loss: 0.0129\nEpoch 36: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0164 - loss: 0.0140 - offset_loss: 0.0128 - onset_loss: 0.0128 - val_frame_loss: 0.0841 - val_loss: 0.0688 - val_offset_loss: 0.0634 - val_onset_loss: 0.0589\nEpoch 37/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0150 - loss: 0.0129 - offset_loss: 0.0119 - onset_loss: 0.0120\nEpoch 37: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0150 - loss: 0.0129 - offset_loss: 0.0119 - onset_loss: 0.0120 - val_frame_loss: 0.0901 - val_loss: 0.0704 - val_offset_loss: 0.0626 - val_onset_loss: 0.0584\nEpoch 38/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0138 - loss: 0.0120 - offset_loss: 0.0111 - onset_loss: 0.0112\nEpoch 38: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0138 - loss: 0.0120 - offset_loss: 0.0111 - onset_loss: 0.0112 - val_frame_loss: 0.0836 - val_loss: 0.0666 - val_offset_loss: 0.0615 - val_onset_loss: 0.0548\nEpoch 39/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0133 - loss: 0.0118 - offset_loss: 0.0110 - onset_loss: 0.0110\nEpoch 39: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0133 - loss: 0.0118 - offset_loss: 0.0110 - onset_loss: 0.0110 - val_frame_loss: 0.0848 - val_loss: 0.0673 - val_offset_loss: 0.0602 - val_onset_loss: 0.0570\nEpoch 40/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0134 - loss: 0.0119 - offset_loss: 0.0111 - onset_loss: 0.0112\nEpoch 40: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 3s/step - frame_loss: 0.0134 - loss: 0.0119 - offset_loss: 0.0111 - onset_loss: 0.0112 - val_frame_loss: 0.0877 - val_loss: 0.0672 - val_offset_loss: 0.0532 - val_onset_loss: 0.0607\nEpoch 41/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0126 - loss: 0.0112 - offset_loss: 0.0105 - onset_loss: 0.0106\nEpoch 41: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0126 - loss: 0.0112 - offset_loss: 0.0105 - onset_loss: 0.0106 - val_frame_loss: 0.0843 - val_loss: 0.0649 - val_offset_loss: 0.0537 - val_onset_loss: 0.0567\nEpoch 42/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0127 - loss: 0.0114 - offset_loss: 0.0108 - onset_loss: 0.0108\nEpoch 42: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - frame_loss: 0.0126 - loss: 0.0114 - offset_loss: 0.0108 - onset_loss: 0.0108 - val_frame_loss: 0.0900 - val_loss: 0.0683 - val_offset_loss: 0.0582 - val_onset_loss: 0.0568\nEpoch 43/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0111 - loss: 0.0103 - offset_loss: 0.0098 - onset_loss: 0.0100\nEpoch 43: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 3s/step - frame_loss: 0.0111 - loss: 0.0103 - offset_loss: 0.0098 - onset_loss: 0.0100 - val_frame_loss: 0.0885 - val_loss: 0.0708 - val_offset_loss: 0.0534 - val_onset_loss: 0.0704\nEpoch 44/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0111 - loss: 0.0103 - offset_loss: 0.0099 - onset_loss: 0.0099\nEpoch 44: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 3s/step - frame_loss: 0.0111 - loss: 0.0103 - offset_loss: 0.0099 - onset_loss: 0.0099 - val_frame_loss: 0.0926 - val_loss: 0.0712 - val_offset_loss: 0.0549 - val_onset_loss: 0.0661\nEpoch 45/100\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - frame_loss: 0.0103 - loss: 0.0097 - offset_loss: 0.0094 - onset_loss: 0.0094\nEpoch 45: val_loss did not improve from 0.06071\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 3s/step - frame_loss: 0.0103 - loss: 0.0097 - offset_loss: 0.0094 - onset_loss: 0.0094 - val_frame_loss: 0.0945 - val_loss: 0.0702 - val_offset_loss: 0.0555 - val_onset_loss: 0.0606\nEpoch 45: early stopping\nRestoring model weights from the end of the best epoch: 25.\n\n🎯 Frame-level Metrics:\nPrecision: 0.7274\nRecall: 0.7026\nF1-score: 0.7148\n\nFrame F1 Score is 0.7148, below CR-GCN's 92.77%. Consider further optimization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecay\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport logging\nimport csv\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import median_filter\nimport pandas as pd\n\n# Suppress TensorFlow CUDA warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Constants\nSR = 16000\nN_MELS = 229\nHOP_LENGTH = 512\nTARGET_FRAMES = 480  # Consider reducing if memory issues persist\nNOTES = 88\nINPUT_DIR = \"/kaggle/input\"\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"preprocessed\", \"preprocessed\")\nWORKING_DIR = \"/kaggle/working\"\nBATCH_SIZE = 2  # Reduced from 8 to lower memory usage\nITERATIONS = 2400\nCHECKPOINT_INTERVAL = 240\nCSV_FILE = os.path.join(WORKING_DIR, \"training_metrics.csv\")\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify directory structure\ndef verify_data_dirs():\n    expected_dirs = [\n        os.path.join(OUTPUT_DIR, 'train', 'mel'),\n        os.path.join(OUTPUT_DIR, 'train', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'validation', 'mel'),\n        os.path.join(OUTPUT_DIR, 'validation', 'piano_roll'),\n        os.path.join(OUTPUT_DIR, 'test', 'mel'),\n        os.path.join(OUTPUT_DIR, 'test', 'piano_roll')\n    ]\n    for dir_path in expected_dirs:\n        if not os.path.exists(dir_path):\n            logger.error(f\"Directory {dir_path} not found.\")\n            raise FileNotFoundError(f\"Directory {dir_path} not found.\")\n\n# Normalize adjacency matrix\ndef normalize_adj(A, symmetric=True):\n    A = A + tf.eye(tf.shape(A)[0], dtype=tf.float32)\n    d = tf.reduce_sum(A, axis=1)\n    if symmetric:\n        D_inv_sqrt = tf.linalg.diag(tf.pow(d, -0.5))\n        return D_inv_sqrt @ A @ D_inv_sqrt\n    else:\n        D_inv = tf.linalg.diag(tf.pow(d, -1))\n        return D_inv @ A\n\n# Compute adjacency matrix\ndef compute_adjacency_matrix(train_roll_dir):\n    train_rolls = []\n    for f in os.listdir(train_roll_dir):\n        if f.endswith('.npy'):\n            roll = np.load(os.path.join(train_roll_dir, f))\n            train_rolls.append(roll)\n    all_frames = np.concatenate(train_rolls, axis=1)\n    all_frames = (all_frames > 0).astype(np.float32)\n    M = tf.matmul(all_frames, all_frames, transpose_b=True)\n    M = M + tf.transpose(M)\n    M_max = tf.reduce_max(M)\n    P = M / M_max if M_max > 0 else M\n    A = tf.cast(P >= 0.6, tf.float32)\n    A = tf.cast((A + tf.transpose(A)) > 0, tf.float32)\n    A = normalize_adj(A, symmetric=True)\n    return A\n\n# GCN Layer\nclass GCNSimple(tf.keras.layers.Layer):\n    def __init__(self, dim_in, dim_out, **kwargs):\n        super(GCNSimple, self).__init__(**kwargs)\n        train_roll_dir = os.path.join(OUTPUT_DIR, 'train', 'piano_roll')\n        self.A = tf.constant(compute_adjacency_matrix(train_roll_dir), dtype=tf.float32)\n        self.fc1 = layers.Dense(dim_in, use_bias=False)\n        self.fc2 = layers.Dense(dim_out // 2, use_bias=False)\n        self.fc3 = layers.Dense(dim_out, use_bias=False)\n    \n    def call(self, inputs):\n        X = tf.nn.relu(self.fc1(self.A))\n        X = tf.nn.relu(self.fc2(tf.matmul(self.A, X)))\n        X = self.fc3(tf.matmul(self.A, X))\n        return tf.matmul(inputs, X)\n    \n    def get_config(self):\n        config = super(GCNSimple, self).get_config()\n        config.update({'dim_in': self.fc1.units, 'dim_out': self.fc3.units})\n        return config\n\n# ConvStack\ndef build_conv_stack(input_features, output_features):\n    model = models.Sequential([\n        layers.Input(shape=(None, input_features, 1)),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Conv2D(output_features // 16, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Dropout(0.4),\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.MaxPooling2D((1, 2)),\n        layers.Conv2D(output_features // 8, (3, 3), padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Reshape((-1, (output_features // 8) * (input_features // 4))),\n        layers.Dense(output_features, kernel_regularizer=tf.keras.regularizers.l2(0.02)),\n        layers.Dropout(0.4)\n    ])\n    return model\n\n# OnsetsAndFrames model with Input layer\nclass OnsetsAndFrames(Model):\n    def __init__(self, input_features, output_features, model_complexity=48, **kwargs):  # Reduced from 96 to 48\n        super(OnsetsAndFrames, self).__init__(**kwargs)\n        self.input_features = input_features\n        self.output_features = output_features\n        self.model_complexity = model_complexity\n        model_size = model_complexity * 16\n        \n        # Onset stack\n        onset_input = layers.Input(shape=(None, input_features, 1))\n        x = build_conv_stack(input_features, model_size)(onset_input)\n        x = layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True))(x)\n        x = layers.Dropout(0.3)(x)\n        onset_output = layers.Dense(output_features, activation='sigmoid')(x)\n        self.onset_stack = models.Model(onset_input, onset_output)\n        \n        # Offset stack\n        offset_input = layers.Input(shape=(None, input_features, 1))\n        x = build_conv_stack(input_features, model_size)(offset_input)\n        x = layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True))(x)\n        x = layers.Dropout(0.3)(x)\n        offset_output = layers.Dense(output_features, activation='sigmoid')(x)\n        self.offset_stack = models.Model(offset_input, offset_output)\n        \n        # Frame stack\n        frame_input = layers.Input(shape=(None, input_features, 1))\n        x = build_conv_stack(input_features, model_size)(frame_input)\n        frame_output = layers.Dense(output_features, activation='sigmoid')(x)\n        self.frame_stack = models.Model(frame_input, frame_output)\n        \n        # Combined stack\n        combined_input = layers.Input(shape=(None, output_features * 3))\n        x = layers.Bidirectional(layers.LSTM(model_size // 2, return_sequences=True))(combined_input)\n        x = layers.Dense(output_features)(x)\n        x = GCNSimple(output_features, output_features)(x)\n        combined_output = layers.Activation('sigmoid')(x)\n        self.combined_stack = models.Model(combined_input, combined_output)\n    \n    def call(self, mel, training=False):\n        mel = tf.expand_dims(mel, -1)\n        onset_pred = self.onset_stack(mel, training=training)\n        offset_pred = self.offset_stack(mel, training=training)\n        activation_pred = self.frame_stack(mel, training=training)\n        combined_pred = tf.concat([onset_pred, offset_pred, activation_pred], axis=-1)\n        frame_pred = self.combined_stack(combined_pred, training=training)\n        return {'onset': onset_pred, 'offset': offset_pred, 'frame': frame_pred}\n    \n    def get_config(self):\n        config = super(OnsetsAndFrames, self).get_config()\n        config.update({'input_features': self.input_features, 'output_features': self.output_features, 'model_complexity': self.model_complexity})\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n# Data generator\nclass PianoDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, split, batch_size=2, shuffle=False):  # Reduced batch size\n        self.split = split\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.audio_dir = os.path.join(OUTPUT_DIR, split, 'mel')\n        self.roll_dir = os.path.join(OUTPUT_DIR, split, 'piano_roll')\n        self.audio_files = [f for f in os.listdir(self.audio_dir) if f.endswith('.npy')]\n        self.roll_files = [f for f in os.listdir(self.roll_dir) if f.endswith('.npy')]\n        self.audio_files.sort()\n        self.roll_files.sort()\n        assert len(self.audio_files) == len(self.roll_files), f\"Mismatch in {split}\"\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.audio_files) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_audio_files = self.audio_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_roll_files = self.roll_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.array([np.load(os.path.join(self.audio_dir, f)) for f in batch_audio_files])\n        y = np.array([np.load(os.path.join(self.roll_dir, f)) for f in batch_roll_files])\n        \n        if X.ndim == 2:\n            X = np.expand_dims(X, axis=0)\n        if y.ndim == 2:\n            y = np.expand_dims(y, axis=0)\n        \n        # Normalize and augment\n        X = (X - np.min(X)) / (np.max(X) - np.min(X) + 1e-8)\n        X = X + np.random.normal(0, 0.01, X.shape)  # Noise\n        X = np.clip(X * np.random.uniform(0.9, 1.1, X.shape), 0, 1)  # Time stretching effect\n        \n        if X.shape[1] == N_MELS and X.shape[2] == TARGET_FRAMES:\n            X = np.transpose(X, (0, 2, 1))\n        if y.shape[1] == NOTES and y.shape[2] == TARGET_FRAMES:\n            y = np.transpose(y, (0, 2, 1))\n        \n        logger.info(f\"X shape: {X.shape}, y shape: {y.shape}\")\n        \n        return X, {'onset': y, 'offset': y, 'frame': y}\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            indices = np.arange(len(self.audio_files))\n            np.random.shuffle(indices)\n            self.audio_files = [self.audio_files[i] for i in indices]\n            self.roll_files = [self.roll_files[i] for i in indices]\n\n# Custom training step with manual binary cross-entropy\nclass CustomModel(Model):\n    def __init__(self, onsets_and_frames_model):\n        super(CustomModel, self).__init__()\n        self.onsets_and_frames = onsets_and_frames_model\n        self.clip_norm = 1.0\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.onset_loss_tracker = tf.keras.metrics.Mean(name=\"onset_loss\")\n        self.offset_loss_tracker = tf.keras.metrics.Mean(name=\"offset_loss\")\n        self.frame_loss_tracker = tf.keras.metrics.Mean(name=\"frame_loss\")\n        self.val_loss_tracker = tf.keras.metrics.Mean(name=\"val_loss\")\n        self.val_onset_loss_tracker = tf.keras.metrics.Mean(name=\"val_onset_loss\")\n        self.val_offset_loss_tracker = tf.keras.metrics.Mean(name=\"val_offset_loss\")\n        self.val_frame_loss_tracker = tf.keras.metrics.Mean(name=\"val_frame_loss\")\n    \n    def call(self, inputs, training=False):\n        return self.onsets_and_frames(inputs, training=training)\n    \n    def compute_loss(self, x, y, y_pred, training=True):\n        losses = {}\n        for key in ['onset', 'offset', 'frame']:\n            y_true = tf.cast(y[key], tf.float32)\n            y_pred_key = y_pred[key]\n            # Compute class weights dynamically\n            pos_weight = tf.reduce_mean(y_true)  # Mean of positive class across all dimensions\n            neg_weight = 1 - pos_weight\n            # Create weight tensor with the same shape as y_true\n            weights = tf.where(y_true > 0, pos_weight / (pos_weight + 1e-8), neg_weight / (neg_weight + 1e-8))\n            # Manual binary cross-entropy\n            epsilon = 1e-7\n            y_pred_key = tf.clip_by_value(y_pred_key, epsilon, 1 - epsilon)\n            bce = - (y_true * tf.math.log(y_pred_key) + (1 - y_true) * tf.math.log(1 - y_pred_key))\n            # Apply weights element-wise\n            weighted_loss = bce * tf.cast(weights, tf.float32)\n            losses[key] = tf.reduce_mean(weighted_loss)\n            logger.info(f\"Shapes - y_true: {y_true.shape}, y_pred_key: {y_pred_key.shape}, weights: {weights.shape}, bce: {bce.shape}, weighted_loss: {weighted_loss.shape}\")\n        total_loss = sum(losses.values()) / len(losses)\n        return total_loss, losses['onset'], losses['offset'], losses['frame']\n    \n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred, training=True)\n        \n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(total_loss, trainable_vars)\n        gradients = [tf.clip_by_norm(g, self.clip_norm) if g is not None else g for g in gradients]\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        self.loss_tracker.update_state(total_loss)\n        self.onset_loss_tracker.update_state(onset_loss)\n        self.offset_loss_tracker.update_state(offset_loss)\n        self.frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.loss_tracker.result(),\n            \"onset_loss\": self.onset_loss_tracker.result(),\n            \"offset_loss\": self.offset_loss_tracker.result(),\n            \"frame_loss\": self.frame_loss_tracker.result()\n        }\n    \n    def test_step(self, data):\n        x, y = data\n        y_pred = self(x, training=False)\n        total_loss, onset_loss, offset_loss, frame_loss = self.compute_loss(x, y, y_pred, training=False)\n        \n        self.val_loss_tracker.update_state(total_loss)\n        self.val_onset_loss_tracker.update_state(onset_loss)\n        self.val_offset_loss_tracker.update_state(offset_loss)\n        self.val_frame_loss_tracker.update_state(frame_loss)\n        \n        return {\n            \"loss\": self.val_loss_tracker.result(),\n            \"onset_loss\": self.val_onset_loss_tracker.result(),\n            \"offset_loss\": self.val_offset_loss_tracker.result(),\n            \"frame_loss\": self.val_frame_loss_tracker.result()\n        }\n    \n    @property\n    def metrics(self):\n        return [\n            self.loss_tracker, self.onset_loss_tracker, self.offset_loss_tracker, self.frame_loss_tracker,\n            self.val_loss_tracker, self.val_onset_loss_tracker, self.val_offset_loss_tracker, self.val_frame_loss_tracker\n        ]\n\n# Training function\ndef train_model():\n    train_gen = PianoDataGenerator('train', batch_size=BATCH_SIZE)\n    val_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE)\n    \n    batches_per_epoch = len(train_gen)\n    total_epochs = 100\n    logger.info(f\"Training for {total_epochs} epochs with {batches_per_epoch} batches/epoch\")\n    \n    onsets_and_frames = OnsetsAndFrames(input_features=N_MELS, output_features=NOTES)\n    model = CustomModel(onsets_and_frames)\n    \n    lr_schedule = CosineDecay(\n        initial_learning_rate=0.0001,\n        decay_steps=total_epochs * batches_per_epoch\n    )\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=None,\n        metrics=None\n    )\n    \n    checkpoint = ModelCheckpoint(\n        os.path.join(WORKING_DIR, \"best_model.keras\"),\n        monitor=\"val_loss\",\n        save_best_only=True,\n        verbose=1\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor=\"val_loss\",\n        patience=30,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    class CSVMetricsCallback(tf.keras.callbacks.Callback):\n        def __init__(self, csv_file):\n            super().__init__()\n            self.csv_file = csv_file\n            if os.path.exists(csv_file):\n                os.remove(csv_file)\n            with open(csv_file, 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow([\"epoch\", \"loss\", \"onset_loss\", \"offset_loss\", \"frame_loss\", \n                               \"val_loss\", \"val_onset_loss\", \"val_offset_loss\", \"val_frame_loss\"])\n        \n        def on_epoch_end(self, epoch, logs=None):\n            logs = logs or {}\n            row = [epoch + 1, logs.get('loss', 0), logs.get('onset_loss', 0), logs.get('offset_loss', 0), \n                   logs.get('frame_loss', 0), logs.get('val_loss', 0), logs.get('val_onset_loss', 0), \n                   logs.get('val_offset_loss', 0), logs.get('val_frame_loss', 0)]\n            with open(self.csv_file, 'a', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerow(row)\n    \n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=total_epochs,\n        callbacks=[checkpoint, early_stopping, CSVMetricsCallback(CSV_FILE)],\n        verbose=1\n    )\n    \n    return model, history\n\n# Evaluation function with threshold tuning\ndef evaluate_model(model, test_gen):\n    max_samples = min(10, len(test_gen))\n    Y_true_all, Y_pred_all = [], []\n    best_f1 = 0\n    best_threshold = 0.5\n    \n    for threshold in [0.2, 0.3, 0.4, 0.5]:\n        Y_true_all, Y_pred_all = [], []\n        for i in range(max_samples):\n            try:\n                batch_x, batch_y = test_gen[i]\n                batch_y = batch_y['frame']\n            except ValueError as e:\n                print(f\"⏭️ Skipping batch {i}: {e}\")\n                continue\n            \n            preds = model.predict(batch_x, verbose=0)\n            pred_frame = preds['frame']\n            pred_frame = np.where(pred_frame > threshold, 1, 0)\n            pred_frame = median_filter(pred_frame, size=(1, 3))  # Post-processing\n            \n            Y_pred_all.append(pred_frame[0] if BATCH_SIZE == 1 else pred_frame)\n            Y_true_all.append(batch_y[0] if BATCH_SIZE == 1 else batch_y)\n        \n        Y_true_flat = np.concatenate(Y_true_all).reshape(-1)\n        Y_pred_flat = np.concatenate(Y_pred_all).reshape(-1)\n        \n        frame_precision = precision_score(Y_true_flat, Y_pred_flat, zero_division=0)\n        frame_recall = recall_score(Y_true_flat, Y_pred_flat, zero_division=0)\n        frame_f1 = f1_score(Y_true_flat, Y_pred_flat, zero_division=0)\n        \n        print(f\"\\n🎯 Frame-level Metrics (Threshold {threshold}):\")\n        print(f\"Precision: {frame_precision:.4f}\")\n        print(f\"Recall: {frame_recall:.4f}\")\n        print(f\"F1-score: {frame_f1:.4f}\")\n        \n        if frame_f1 > best_f1:\n            best_f1 = frame_f1\n            best_threshold = threshold\n    \n    print(f\"\\nBest F1-score: {best_f1:.4f} at threshold {best_threshold}\")\n    if best_f1 >= 0.9277:\n        print(\"\\nFrame F1 Score exceeds CR-GCN's 92.77% - Excellent!\")\n    else:\n        print(f\"\\nFrame F1 Score is {best_f1:.4f}, below CR-GCN's 92.77%. Consider further optimization.\")\n    \n    return best_f1\n\n# Plot training vs validation loss\ndef plot_training_validation_curve(csv_file):\n    if not os.path.exists(csv_file):\n        print(f\"Error: {csv_file} not found.\")\n        return\n    df = pd.read_csv(csv_file)\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['epoch'], df['loss'], label='Training Loss')\n    plt.plot(df['epoch'], df['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training vs Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(WORKING_DIR, 'training_validation_loss.png'))\n    plt.show()\n\nif __name__ == '__main__':\n    verify_data_dirs()\n    model, history = train_model()\n    test_gen = PianoDataGenerator('validation', batch_size=BATCH_SIZE, shuffle=False)\n    evaluate_model(model, test_gen)\n    model.save(os.path.join(WORKING_DIR, \"final_model.keras\"))\n    logger.info(f\"Saved final model to {os.path.join(WORKING_DIR, 'final_model.keras')}\")\n    plot_training_validation_curve(CSV_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:43:03.048745Z","iopub.execute_input":"2025-04-20T21:43:03.049027Z","iopub.status.idle":"2025-04-21T00:55:14.264768Z","shell.execute_reply.started":"2025-04-20T21:43:03.048997Z","shell.execute_reply":"2025-04-21T00:55:14.263505Z"}},"outputs":[{"name":"stderr","text":"2025-04-20 21:43:06.007833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745185386.422528      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745185386.546205      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1745185405.625015      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745185405.625741      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745185441.022415      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/custom_model_1/onsets_and_frames_1/functional_1_1/sequential_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nI0000 00:00:1745185442.853952      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - frame_loss: 0.3399 - loss: 0.2425 - offset_loss: 0.1924 - onset_loss: 0.1951\nEpoch 1: val_loss improved from inf to 0.15723, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 431ms/step - frame_loss: 0.3397 - loss: 0.2423 - offset_loss: 0.1923 - onset_loss: 0.1950 - val_frame_loss: 0.1483 - val_loss: 0.1572 - val_offset_loss: 0.1605 - val_onset_loss: 0.1629\nEpoch 2/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.1560 - loss: 0.1166 - offset_loss: 0.0967 - onset_loss: 0.0970\nEpoch 2: val_loss improved from 0.15723 to 0.09940, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 433ms/step - frame_loss: 0.1560 - loss: 0.1166 - offset_loss: 0.0967 - onset_loss: 0.0970 - val_frame_loss: 0.1411 - val_loss: 0.0994 - val_offset_loss: 0.0796 - val_onset_loss: 0.0775\nEpoch 3/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.1442 - loss: 0.0999 - offset_loss: 0.0781 - onset_loss: 0.0774\nEpoch 3: val_loss did not improve from 0.09940\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.1442 - loss: 0.0999 - offset_loss: 0.0780 - onset_loss: 0.0774 - val_frame_loss: 0.1201 - val_loss: 0.1007 - val_offset_loss: 0.0919 - val_onset_loss: 0.0902\nEpoch 4/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.1181 - loss: 0.0827 - offset_loss: 0.0652 - onset_loss: 0.0647\nEpoch 4: val_loss improved from 0.09940 to 0.08828, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 433ms/step - frame_loss: 0.1181 - loss: 0.0827 - offset_loss: 0.0652 - onset_loss: 0.0647 - val_frame_loss: 0.1271 - val_loss: 0.0883 - val_offset_loss: 0.0695 - val_onset_loss: 0.0682\nEpoch 5/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.1024 - loss: 0.0727 - offset_loss: 0.0582 - onset_loss: 0.0575\nEpoch 5: val_loss improved from 0.08828 to 0.07971, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 433ms/step - frame_loss: 0.1024 - loss: 0.0727 - offset_loss: 0.0582 - onset_loss: 0.0575 - val_frame_loss: 0.1002 - val_loss: 0.0797 - val_offset_loss: 0.0708 - val_onset_loss: 0.0681\nEpoch 6/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0913 - loss: 0.0665 - offset_loss: 0.0545 - onset_loss: 0.0538\nEpoch 6: val_loss improved from 0.07971 to 0.07634, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 433ms/step - frame_loss: 0.0913 - loss: 0.0665 - offset_loss: 0.0545 - onset_loss: 0.0538 - val_frame_loss: 0.0934 - val_loss: 0.0763 - val_offset_loss: 0.0688 - val_onset_loss: 0.0669\nEpoch 7/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0787 - loss: 0.0586 - offset_loss: 0.0489 - onset_loss: 0.0483\nEpoch 7: val_loss did not improve from 0.07634\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0787 - loss: 0.0586 - offset_loss: 0.0489 - onset_loss: 0.0483 - val_frame_loss: 0.0862 - val_loss: 0.0764 - val_offset_loss: 0.0695 - val_onset_loss: 0.0735\nEpoch 8/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0699 - loss: 0.0539 - offset_loss: 0.0464 - onset_loss: 0.0456\nEpoch 8: val_loss did not improve from 0.07634\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0699 - loss: 0.0540 - offset_loss: 0.0464 - onset_loss: 0.0456 - val_frame_loss: 0.0879 - val_loss: 0.0771 - val_offset_loss: 0.0691 - val_onset_loss: 0.0743\nEpoch 9/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0644 - loss: 0.0508 - offset_loss: 0.0443 - onset_loss: 0.0438\nEpoch 9: val_loss did not improve from 0.07634\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0644 - loss: 0.0508 - offset_loss: 0.0443 - onset_loss: 0.0438 - val_frame_loss: 0.0961 - val_loss: 0.0873 - val_offset_loss: 0.0771 - val_onset_loss: 0.0885\nEpoch 10/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0599 - loss: 0.0487 - offset_loss: 0.0434 - onset_loss: 0.0429\nEpoch 10: val_loss improved from 0.07634 to 0.06931, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0599 - loss: 0.0487 - offset_loss: 0.0434 - onset_loss: 0.0429 - val_frame_loss: 0.0820 - val_loss: 0.0693 - val_offset_loss: 0.0616 - val_onset_loss: 0.0644\nEpoch 11/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0535 - loss: 0.0442 - offset_loss: 0.0396 - onset_loss: 0.0394\nEpoch 11: val_loss improved from 0.06931 to 0.06667, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0535 - loss: 0.0442 - offset_loss: 0.0396 - onset_loss: 0.0394 - val_frame_loss: 0.0794 - val_loss: 0.0667 - val_offset_loss: 0.0599 - val_onset_loss: 0.0607\nEpoch 12/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0483 - loss: 0.0412 - offset_loss: 0.0378 - onset_loss: 0.0374\nEpoch 12: val_loss improved from 0.06667 to 0.06562, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0483 - loss: 0.0412 - offset_loss: 0.0378 - onset_loss: 0.0374 - val_frame_loss: 0.0791 - val_loss: 0.0656 - val_offset_loss: 0.0591 - val_onset_loss: 0.0586\nEpoch 13/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0459 - loss: 0.0396 - offset_loss: 0.0368 - onset_loss: 0.0362\nEpoch 13: val_loss improved from 0.06562 to 0.06516, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0459 - loss: 0.0396 - offset_loss: 0.0368 - onset_loss: 0.0362 - val_frame_loss: 0.0747 - val_loss: 0.0652 - val_offset_loss: 0.0600 - val_onset_loss: 0.0608\nEpoch 14/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0412 - loss: 0.0365 - offset_loss: 0.0344 - onset_loss: 0.0341\nEpoch 14: val_loss did not improve from 0.06516\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0412 - loss: 0.0365 - offset_loss: 0.0344 - onset_loss: 0.0341 - val_frame_loss: 0.0764 - val_loss: 0.0697 - val_offset_loss: 0.0725 - val_onset_loss: 0.0602\nEpoch 15/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0402 - loss: 0.0362 - offset_loss: 0.0345 - onset_loss: 0.0340\nEpoch 15: val_loss did not improve from 0.06516\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0402 - loss: 0.0362 - offset_loss: 0.0345 - onset_loss: 0.0340 - val_frame_loss: 0.0762 - val_loss: 0.0671 - val_offset_loss: 0.0633 - val_onset_loss: 0.0617\nEpoch 16/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0361 - loss: 0.0331 - offset_loss: 0.0316 - onset_loss: 0.0314\nEpoch 16: val_loss improved from 0.06516 to 0.06394, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 433ms/step - frame_loss: 0.0361 - loss: 0.0331 - offset_loss: 0.0316 - onset_loss: 0.0314 - val_frame_loss: 0.0736 - val_loss: 0.0639 - val_offset_loss: 0.0606 - val_onset_loss: 0.0577\nEpoch 17/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0365 - loss: 0.0337 - offset_loss: 0.0323 - onset_loss: 0.0322\nEpoch 17: val_loss improved from 0.06394 to 0.06312, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0365 - loss: 0.0337 - offset_loss: 0.0323 - onset_loss: 0.0322 - val_frame_loss: 0.0727 - val_loss: 0.0631 - val_offset_loss: 0.0594 - val_onset_loss: 0.0573\nEpoch 18/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0341 - loss: 0.0323 - offset_loss: 0.0314 - onset_loss: 0.0314\nEpoch 18: val_loss did not improve from 0.06312\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0341 - loss: 0.0323 - offset_loss: 0.0314 - onset_loss: 0.0314 - val_frame_loss: 0.0726 - val_loss: 0.0635 - val_offset_loss: 0.0594 - val_onset_loss: 0.0585\nEpoch 19/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0311 - loss: 0.0298 - offset_loss: 0.0294 - onset_loss: 0.0289\nEpoch 19: val_loss did not improve from 0.06312\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0311 - loss: 0.0298 - offset_loss: 0.0294 - onset_loss: 0.0289 - val_frame_loss: 0.0740 - val_loss: 0.0634 - val_offset_loss: 0.0587 - val_onset_loss: 0.0576\nEpoch 20/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0312 - loss: 0.0302 - offset_loss: 0.0298 - onset_loss: 0.0296\nEpoch 20: val_loss improved from 0.06312 to 0.06276, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0312 - loss: 0.0302 - offset_loss: 0.0298 - onset_loss: 0.0296 - val_frame_loss: 0.0718 - val_loss: 0.0628 - val_offset_loss: 0.0577 - val_onset_loss: 0.0588\nEpoch 21/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0286 - loss: 0.0282 - offset_loss: 0.0280 - onset_loss: 0.0278\nEpoch 21: val_loss did not improve from 0.06276\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0286 - loss: 0.0282 - offset_loss: 0.0280 - onset_loss: 0.0278 - val_frame_loss: 0.0755 - val_loss: 0.0637 - val_offset_loss: 0.0584 - val_onset_loss: 0.0571\nEpoch 22/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0271 - loss: 0.0272 - offset_loss: 0.0274 - onset_loss: 0.0273\nEpoch 22: val_loss did not improve from 0.06276\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0271 - loss: 0.0272 - offset_loss: 0.0274 - onset_loss: 0.0273 - val_frame_loss: 0.0798 - val_loss: 0.0660 - val_offset_loss: 0.0600 - val_onset_loss: 0.0581\nEpoch 23/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0270 - loss: 0.0272 - offset_loss: 0.0275 - onset_loss: 0.0271\nEpoch 23: val_loss did not improve from 0.06276\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0270 - loss: 0.0272 - offset_loss: 0.0275 - onset_loss: 0.0271 - val_frame_loss: 0.0772 - val_loss: 0.0647 - val_offset_loss: 0.0578 - val_onset_loss: 0.0590\nEpoch 24/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0238 - loss: 0.0245 - offset_loss: 0.0250 - onset_loss: 0.0248\nEpoch 24: val_loss improved from 0.06276 to 0.06266, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 433ms/step - frame_loss: 0.0238 - loss: 0.0246 - offset_loss: 0.0250 - onset_loss: 0.0248 - val_frame_loss: 0.0725 - val_loss: 0.0627 - val_offset_loss: 0.0578 - val_onset_loss: 0.0576\nEpoch 25/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0235 - loss: 0.0244 - offset_loss: 0.0251 - onset_loss: 0.0248\nEpoch 25: val_loss improved from 0.06266 to 0.06184, saving model to /kaggle/working/best_model.keras\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 434ms/step - frame_loss: 0.0235 - loss: 0.0244 - offset_loss: 0.0251 - onset_loss: 0.0248 - val_frame_loss: 0.0729 - val_loss: 0.0618 - val_offset_loss: 0.0566 - val_onset_loss: 0.0561\nEpoch 26/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0235 - loss: 0.0244 - offset_loss: 0.0249 - onset_loss: 0.0248\nEpoch 26: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0235 - loss: 0.0244 - offset_loss: 0.0249 - onset_loss: 0.0248 - val_frame_loss: 0.0767 - val_loss: 0.0634 - val_offset_loss: 0.0568 - val_onset_loss: 0.0569\nEpoch 27/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0218 - loss: 0.0229 - offset_loss: 0.0234 - onset_loss: 0.0235\nEpoch 27: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0218 - loss: 0.0229 - offset_loss: 0.0234 - onset_loss: 0.0235 - val_frame_loss: 0.0846 - val_loss: 0.0687 - val_offset_loss: 0.0612 - val_onset_loss: 0.0602\nEpoch 28/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0222 - loss: 0.0235 - offset_loss: 0.0242 - onset_loss: 0.0241\nEpoch 28: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0222 - loss: 0.0235 - offset_loss: 0.0242 - onset_loss: 0.0241 - val_frame_loss: 0.0777 - val_loss: 0.0645 - val_offset_loss: 0.0575 - val_onset_loss: 0.0583\nEpoch 29/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0205 - loss: 0.0220 - offset_loss: 0.0228 - onset_loss: 0.0228\nEpoch 29: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0205 - loss: 0.0220 - offset_loss: 0.0228 - onset_loss: 0.0228 - val_frame_loss: 0.0841 - val_loss: 0.0679 - val_offset_loss: 0.0604 - val_onset_loss: 0.0591\nEpoch 30/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0211 - loss: 0.0225 - offset_loss: 0.0232 - onset_loss: 0.0232\nEpoch 30: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0211 - loss: 0.0225 - offset_loss: 0.0232 - onset_loss: 0.0232 - val_frame_loss: 0.0807 - val_loss: 0.0670 - val_offset_loss: 0.0628 - val_onset_loss: 0.0574\nEpoch 31/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0199 - loss: 0.0217 - offset_loss: 0.0228 - onset_loss: 0.0224\nEpoch 31: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0199 - loss: 0.0217 - offset_loss: 0.0228 - onset_loss: 0.0224 - val_frame_loss: 0.0770 - val_loss: 0.0634 - val_offset_loss: 0.0566 - val_onset_loss: 0.0567\nEpoch 32/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0186 - loss: 0.0206 - offset_loss: 0.0217 - onset_loss: 0.0214\nEpoch 32: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0186 - loss: 0.0206 - offset_loss: 0.0217 - onset_loss: 0.0214 - val_frame_loss: 0.0894 - val_loss: 0.0712 - val_offset_loss: 0.0654 - val_onset_loss: 0.0588\nEpoch 33/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0175 - loss: 0.0196 - offset_loss: 0.0208 - onset_loss: 0.0206\nEpoch 33: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0175 - loss: 0.0196 - offset_loss: 0.0208 - onset_loss: 0.0206 - val_frame_loss: 0.0821 - val_loss: 0.0661 - val_offset_loss: 0.0577 - val_onset_loss: 0.0586\nEpoch 34/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0173 - loss: 0.0194 - offset_loss: 0.0206 - onset_loss: 0.0205\nEpoch 34: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0173 - loss: 0.0194 - offset_loss: 0.0206 - onset_loss: 0.0205 - val_frame_loss: 0.0866 - val_loss: 0.0688 - val_offset_loss: 0.0593 - val_onset_loss: 0.0604\nEpoch 35/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0164 - loss: 0.0187 - offset_loss: 0.0199 - onset_loss: 0.0198\nEpoch 35: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0164 - loss: 0.0187 - offset_loss: 0.0199 - onset_loss: 0.0198 - val_frame_loss: 0.0809 - val_loss: 0.0667 - val_offset_loss: 0.0575 - val_onset_loss: 0.0618\nEpoch 36/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0162 - loss: 0.0185 - offset_loss: 0.0196 - onset_loss: 0.0195\nEpoch 36: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0162 - loss: 0.0185 - offset_loss: 0.0196 - onset_loss: 0.0195 - val_frame_loss: 0.0884 - val_loss: 0.0683 - val_offset_loss: 0.0594 - val_onset_loss: 0.0572\nEpoch 37/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0161 - loss: 0.0182 - offset_loss: 0.0193 - onset_loss: 0.0191\nEpoch 37: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0161 - loss: 0.0182 - offset_loss: 0.0193 - onset_loss: 0.0191 - val_frame_loss: 0.0825 - val_loss: 0.0657 - val_offset_loss: 0.0588 - val_onset_loss: 0.0557\nEpoch 38/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0150 - loss: 0.0175 - offset_loss: 0.0188 - onset_loss: 0.0186\nEpoch 38: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0150 - loss: 0.0175 - offset_loss: 0.0188 - onset_loss: 0.0186 - val_frame_loss: 0.0804 - val_loss: 0.0657 - val_offset_loss: 0.0590 - val_onset_loss: 0.0577\nEpoch 39/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0155 - loss: 0.0181 - offset_loss: 0.0195 - onset_loss: 0.0193\nEpoch 39: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0155 - loss: 0.0181 - offset_loss: 0.0195 - onset_loss: 0.0193 - val_frame_loss: 0.0835 - val_loss: 0.0663 - val_offset_loss: 0.0592 - val_onset_loss: 0.0563\nEpoch 40/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0140 - loss: 0.0166 - offset_loss: 0.0180 - onset_loss: 0.0178\nEpoch 40: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0140 - loss: 0.0166 - offset_loss: 0.0180 - onset_loss: 0.0178 - val_frame_loss: 0.0896 - val_loss: 0.0692 - val_offset_loss: 0.0600 - val_onset_loss: 0.0579\nEpoch 41/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0139 - loss: 0.0165 - offset_loss: 0.0179 - onset_loss: 0.0178\nEpoch 41: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0139 - loss: 0.0165 - offset_loss: 0.0179 - onset_loss: 0.0178 - val_frame_loss: 0.0876 - val_loss: 0.0682 - val_offset_loss: 0.0591 - val_onset_loss: 0.0580\nEpoch 42/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0135 - loss: 0.0161 - offset_loss: 0.0175 - onset_loss: 0.0173\nEpoch 42: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0135 - loss: 0.0161 - offset_loss: 0.0175 - onset_loss: 0.0173 - val_frame_loss: 0.0884 - val_loss: 0.0694 - val_offset_loss: 0.0605 - val_onset_loss: 0.0594\nEpoch 43/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0127 - loss: 0.0154 - offset_loss: 0.0167 - onset_loss: 0.0167\nEpoch 43: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0127 - loss: 0.0154 - offset_loss: 0.0167 - onset_loss: 0.0167 - val_frame_loss: 0.0880 - val_loss: 0.0695 - val_offset_loss: 0.0591 - val_onset_loss: 0.0613\nEpoch 44/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0134 - loss: 0.0162 - offset_loss: 0.0176 - onset_loss: 0.0175\nEpoch 44: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0134 - loss: 0.0162 - offset_loss: 0.0176 - onset_loss: 0.0175 - val_frame_loss: 0.0866 - val_loss: 0.0682 - val_offset_loss: 0.0600 - val_onset_loss: 0.0578\nEpoch 45/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0129 - loss: 0.0155 - offset_loss: 0.0169 - onset_loss: 0.0168\nEpoch 45: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0129 - loss: 0.0155 - offset_loss: 0.0169 - onset_loss: 0.0168 - val_frame_loss: 0.0893 - val_loss: 0.0690 - val_offset_loss: 0.0591 - val_onset_loss: 0.0587\nEpoch 46/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0124 - loss: 0.0152 - offset_loss: 0.0166 - onset_loss: 0.0165\nEpoch 46: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0124 - loss: 0.0152 - offset_loss: 0.0166 - onset_loss: 0.0165 - val_frame_loss: 0.0898 - val_loss: 0.0694 - val_offset_loss: 0.0603 - val_onset_loss: 0.0582\nEpoch 47/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0123 - loss: 0.0151 - offset_loss: 0.0167 - onset_loss: 0.0165\nEpoch 47: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0123 - loss: 0.0151 - offset_loss: 0.0167 - onset_loss: 0.0165 - val_frame_loss: 0.0902 - val_loss: 0.0695 - val_offset_loss: 0.0596 - val_onset_loss: 0.0588\nEpoch 48/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0121 - loss: 0.0148 - offset_loss: 0.0162 - onset_loss: 0.0162\nEpoch 48: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0121 - loss: 0.0148 - offset_loss: 0.0162 - onset_loss: 0.0162 - val_frame_loss: 0.0968 - val_loss: 0.0725 - val_offset_loss: 0.0619 - val_onset_loss: 0.0588\nEpoch 49/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0115 - loss: 0.0145 - offset_loss: 0.0160 - onset_loss: 0.0159\nEpoch 49: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0115 - loss: 0.0145 - offset_loss: 0.0160 - onset_loss: 0.0159 - val_frame_loss: 0.0943 - val_loss: 0.0715 - val_offset_loss: 0.0598 - val_onset_loss: 0.0603\nEpoch 50/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0111 - loss: 0.0141 - offset_loss: 0.0156 - onset_loss: 0.0155\nEpoch 50: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0111 - loss: 0.0141 - offset_loss: 0.0156 - onset_loss: 0.0155 - val_frame_loss: 0.0927 - val_loss: 0.0705 - val_offset_loss: 0.0598 - val_onset_loss: 0.0591\nEpoch 51/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0108 - loss: 0.0137 - offset_loss: 0.0153 - onset_loss: 0.0151\nEpoch 51: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 431ms/step - frame_loss: 0.0108 - loss: 0.0137 - offset_loss: 0.0153 - onset_loss: 0.0151 - val_frame_loss: 0.0979 - val_loss: 0.0727 - val_offset_loss: 0.0600 - val_onset_loss: 0.0601\nEpoch 52/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0106 - loss: 0.0135 - offset_loss: 0.0150 - onset_loss: 0.0149\nEpoch 52: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0106 - loss: 0.0135 - offset_loss: 0.0150 - onset_loss: 0.0149 - val_frame_loss: 0.0954 - val_loss: 0.0723 - val_offset_loss: 0.0622 - val_onset_loss: 0.0594\nEpoch 53/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - frame_loss: 0.0103 - loss: 0.0132 - offset_loss: 0.0148 - onset_loss: 0.0147\nEpoch 53: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0103 - loss: 0.0132 - offset_loss: 0.0148 - onset_loss: 0.0147 - val_frame_loss: 0.0941 - val_loss: 0.0718 - val_offset_loss: 0.0619 - val_onset_loss: 0.0595\nEpoch 54/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0096 - loss: 0.0126 - offset_loss: 0.0140 - onset_loss: 0.0140\nEpoch 54: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0096 - loss: 0.0126 - offset_loss: 0.0140 - onset_loss: 0.0140 - val_frame_loss: 0.0977 - val_loss: 0.0725 - val_offset_loss: 0.0600 - val_onset_loss: 0.0596\nEpoch 55/100\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - frame_loss: 0.0101 - loss: 0.0131 - offset_loss: 0.0145 - onset_loss: 0.0145\nEpoch 55: val_loss did not improve from 0.06184\n\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 430ms/step - frame_loss: 0.0101 - loss: 0.0131 - offset_loss: 0.0145 - onset_loss: 0.0145 - val_frame_loss: 0.0909 - val_loss: 0.0701 - val_offset_loss: 0.0610 - val_onset_loss: 0.0582\nEpoch 55: early stopping\nRestoring model weights from the end of the best epoch: 25.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4250561041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtest_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPianoDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORKING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_model.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved final model to {os.path.join(WORKING_DIR, 'final_model.keras')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/4250561041.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_gen)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mpred_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mpred_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_frame\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mpred_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedian_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Post-processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mY_pred_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpred_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/ndimage/_filters.py\u001b[0m in \u001b[0;36mmedian_filter\u001b[0;34m(input, size, footprint, output, mode, cval, origin, axes)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \"\"\"\n\u001b[0;32m-> 1684\u001b[0;31m     return _rank_filter(input, 0, size, footprint, output, mode, cval,\n\u001b[0m\u001b[1;32m   1685\u001b[0m                         origin, 'median', axes=axes)\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/ndimage/_filters.py\u001b[0m in \u001b[0;36m_rank_filter\u001b[0;34m(input, rank, size, footprint, output, mode, cval, origin, operation, axes)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no footprint or filter size provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0mfootprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/ndimage/_ni_support.py\u001b[0m in \u001b[0;36m_normalize_sequence\u001b[0;34m(input, rank)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sequence argument must have length equal to input rank\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: sequence argument must have length equal to input rank"],"ename":"RuntimeError","evalue":"sequence argument must have length equal to input rank","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Loss values extracted from training log\ntrain_loss = [\n    0.2345, 0.0915, 0.0662, 0.0540, 0.0496, 0.0429, 0.0365, 0.0362, \n    0.0303, 0.0274, 0.0227, 0.0213, 0.0196, 0.0172, 0.0163, 0.0140, \n    0.0123, 0.0109, 0.0098, 0.0091, 0.0088, 0.0079\n]\nval_loss = [\n    0.1414, 0.3161, 0.0809, 0.0761, 0.0591, 0.0566, 0.0598, 0.0535, \n    0.0523, 0.0538, 0.0521, 0.0516, 0.0543, 0.0536, 0.0519, 0.0531, \n    0.0518, 0.0537, 0.0522, 0.0537, 0.0542, 0.0552\n]\nepochs = list(range(1, 23))\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, train_loss, 'b-', label='Training Loss')\nplt.plot(epochs, val_loss, 'r-', label='Validation Loss')\nplt.title('Training vs. Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\n\n# Save the plot\nplt.savefig('loss_curve.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:02:34.274236Z","iopub.execute_input":"2025-04-21T08:02:34.274973Z","iopub.status.idle":"2025-04-21T08:02:34.599205Z","shell.execute_reply.started":"2025-04-21T08:02:34.274943Z","shell.execute_reply":"2025-04-21T08:02:34.598537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:02:27.191557Z","iopub.execute_input":"2025-04-21T08:02:27.192293Z","iopub.status.idle":"2025-04-21T08:02:27.831945Z","shell.execute_reply.started":"2025-04-21T08:02:27.192245Z","shell.execute_reply":"2025-04-21T08:02:27.831371Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}